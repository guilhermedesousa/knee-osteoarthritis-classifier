{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83cR6_gZbYYH"
      },
      "source": [
        "## Installing and importing necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLsXK1Qq6VA"
      },
      "source": [
        "### CORN loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3j_wVnlILUM",
        "outputId": "d79f8f80-63a4-4a37-bd97-01bdf1de4b2f"
      },
      "outputs": [],
      "source": [
        "!pip install coral-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzicEbjuq9bb"
      },
      "source": [
        "### GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YiDPJJp6dw1",
        "outputId": "2b3492a8-0381-4ba5-96d4-ba1e3ab2d379"
      },
      "outputs": [],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_vjxMwtq_4l"
      },
      "source": [
        "### Pytorch flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uh9PZOC6Iyw",
        "outputId": "91b3561f-9c0f-4d65-8e51-cd07436af3ab"
      },
      "outputs": [],
      "source": [
        "!pip install ptflops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omjxv4T8EZ0I"
      },
      "source": [
        "### Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KNO4WkMEbbo",
        "outputId": "5b7956ee-7f99-480d-86bb-a7a52ba1d933"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4xztqHerHTY"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGgEhzz7abp0"
      },
      "outputs": [],
      "source": [
        "# basic libs\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import json\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "# pytorch libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from ptflops import get_model_complexity_info\n",
        "from transformers import (ViTForImageClassification,\n",
        "                          ViTImageProcessor,\n",
        "                          DeiTForImageClassification,\n",
        "                          DeiTForImageClassificationWithTeacher,\n",
        "                          DeiTImageProcessor,\n",
        "                          AutoFeatureExtractor,\n",
        "                          AutoImageProcessor,\n",
        "                          AutoModelForImageClassification)\n",
        "\n",
        "# coral pytorch\n",
        "from coral_pytorch.losses import corn_loss\n",
        "from coral_pytorch.dataset import corn_label_from_logits\n",
        "\n",
        "# pytorch grad cam\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# plotting libs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             cohen_kappa_score, mean_absolute_error, confusion_matrix, classification_report,\n",
        "                             roc_curve, auc)\n",
        "from sklearn.preprocessing import label_binarize, OneHotEncoder\n",
        "import gc as gc2\n",
        "import timm\n",
        "\n",
        "import optuna\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a-vV3hXlMFH"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UMjqRzSlN76",
        "outputId": "8f7eebd6-2040-4d4e-f6b4-0df056d03efa"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdd6lk6BmIVn"
      },
      "outputs": [],
      "source": [
        "root_folder = '/content/drive/MyDrive/pgc'\n",
        "dataset_prefix = 'preprocessed_dataset'\n",
        "results_folder = 'trained_models'\n",
        "grad_cam_folder = 'grad_cam'\n",
        "nr_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbsBVB1rPoEy"
      },
      "outputs": [],
      "source": [
        "def get_dataset_dir(nr_classes):\n",
        "  return f'{root_folder}/{dataset_prefix}_{nr_classes}_classes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uMz1S0BuMek"
      },
      "outputs": [],
      "source": [
        "def get_results_dir(nr_classes, model_name):\n",
        "  return f'{root_folder}/{results_folder}/{nr_classes}_classes/{model_name}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVta8SOruYNF"
      },
      "outputs": [],
      "source": [
        "def get_grad_cam_dir(nr_classes, criteria):\n",
        "  return f'{root_folder}/{grad_cam_folder}/{nr_classes}_classes/{criteria}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQnlkmjloyg0"
      },
      "outputs": [],
      "source": [
        "def get_output_dir_gradcam(criteria, model_name):\n",
        "  return f'{root_folder}/{grad_cam_folder}/{criteria}/{model_name}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbhPJjmaZ2E"
      },
      "source": [
        "## Setting random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDEPY-Vpasx9"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRurW5rqdzH"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwouZcsGqfL1"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    data_dir = get_dataset_dir(nr_classes)\n",
        "    train_dir = os.path.join(data_dir, 'train')\n",
        "    val_dir = os.path.join(data_dir, 'val')\n",
        "    test_dir = os.path.join(data_dir, 'test')\n",
        "    calib_dit = os.path.join(data_dir, 'calib')\n",
        "    num_classes = nr_classes\n",
        "    max_samples_per_class = 1700 # undersampling for dataset imbalance\n",
        "    batch_size = 28\n",
        "    num_epochs = 60\n",
        "    shuffle = True\n",
        "    feature_extract = False\n",
        "    use_pretrained = True\n",
        "    learning_rate = 0.0001\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    epsilon = 0.05 # 95% confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_7jf6v_rjFn"
      },
      "source": [
        "## Data transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H4gE5xMrim4"
      },
      "outputs": [],
      "source": [
        "def get_transforms(model_name, dataset, processor=None):\n",
        "  if not processor:\n",
        "    image_size = (224, 224) if (model_name != 'inception_v3') else (299, 299)\n",
        "    normalization_mean = [0.485, 0.456, 0.406]\n",
        "    normalization_std = [0.229, 0.224, 0.225]\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    image_size = processor.crop_size['height']\n",
        "    normalization_mean = processor.image_mean\n",
        "    normalization_std = processor.image_std\n",
        "  else:\n",
        "    image_size = processor.size['height']\n",
        "    normalization_mean = processor.image_mean\n",
        "    normalization_std = processor.image_std\n",
        "\n",
        "  data_transforms = {\n",
        "      'train': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'val': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'test': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'calib': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ])\n",
        "  }\n",
        "\n",
        "  return data_transforms[dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-L6PCsXNGpm"
      },
      "source": [
        "### Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAslNFGLNJKO"
      },
      "outputs": [],
      "source": [
        "def undersample_dataset(dataset):\n",
        "  # organize indices by class\n",
        "  class_indices = defaultdict(list)\n",
        "  for idx, (img_path, label) in enumerate(dataset['train'].imgs):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "  # limit the number of samples per class\n",
        "  limited_indices = {}\n",
        "  for label, indices in class_indices.items():\n",
        "    limited_indices[label] = random.sample(indices, min(len(indices), Config.max_samples_per_class))\n",
        "\n",
        "  limited_train_dataset = Subset(dataset['train'], sum(limited_indices.values(), []))\n",
        "  dataset['train'] = limited_train_dataset\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nele5AD7YJaI"
      },
      "source": [
        "## Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K7x1D2TYL8D"
      },
      "outputs": [],
      "source": [
        "def load_dataset(model_name, processor=None, batch_size=Config.batch_size):\n",
        "  dataset = {\n",
        "      x: datasets.ImageFolder(os.path.join(Config.data_dir, x), get_transforms(model_name, x, processor))\n",
        "      for x in ['train', 'val', 'test', 'calib']\n",
        "  }\n",
        "\n",
        "  # applying undersampling\n",
        "  dataset = undersample_dataset(dataset)\n",
        "\n",
        "  dataloaders = {}\n",
        "  for x in ['train', 'val', 'test', 'calib']:\n",
        "    shuffle = True if x == 'train' else False\n",
        "    dataloaders[x] = DataLoader(\n",
        "        dataset[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "  dataset_sizes = {\n",
        "      x: len(dataset[x])\n",
        "      for x in ['train', 'val', 'test', 'calib']\n",
        "  }\n",
        "\n",
        "  return dataloaders, dataset_sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqP9tvSomN3Z"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBSzCJSFmWnh"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, verbose=True, loss_function='cross_entropy'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.min_delta = min_delta\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def __call__(self, val_loss, model, model_name):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model, model_name)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, model_name):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}).  Saving model ...')\n",
        "        model_file = f'{model_name}_{self.loss_function}.pth'\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PM1rnv_hXeu"
      },
      "source": [
        "## Getting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo3x2DnhOqll"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name):\n",
        "  model = None\n",
        "  if model_name == 'resnet34':\n",
        "    model = models.resnet34(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'resnet50':\n",
        "    model = models.resnet50(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'resnet101':\n",
        "    model = models.resnet101(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'vgg16':\n",
        "    model = models.vgg16(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'vgg19':\n",
        "    model = models.vgg19(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'densenet121':\n",
        "    model = models.densenet121(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'densenet169':\n",
        "    model = models.densenet169(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'inception_v3':\n",
        "    model = models.inception_v3(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'swin_b':\n",
        "    model = models.swin_b(pretrained=Config.use_pretrained)\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    model = DeiTForImageClassification.from_pretrained(model_name, num_labels=Config.num_classes, ignore_mismatched_sizes=True)\n",
        "  elif model_name == 'maxvit_t':\n",
        "    model = models.maxvit_t(pretrained=Config.use_pretrained)\n",
        "  elif ('davit' in model_name) or ('gcvit' in model_name):\n",
        "    model = timm.create_model(model_name, pretrained=Config.use_pretrained, num_classes=Config.num_classes)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOTLWJRsQMzM"
      },
      "outputs": [],
      "source": [
        "def set_fc_layer(model, model_name, loss_function):\n",
        "  num_classes = Config.num_classes if loss_function == 'cross_entropy' else Config.num_classes-1\n",
        "\n",
        "  if 'resnet' in model_name:\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  elif 'vgg' in model_name:\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "  elif 'densenet' in model_name:\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "  elif 'inception' in model_name:\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  elif 'swin' in model_name:\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "  elif 'facebook/deit' in model_name:\n",
        "    model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n",
        "  elif 'maxvit_t' in model_name:\n",
        "    model.classifier[5] = nn.Linear(model.classifier[5].in_features, num_classes)\n",
        "  elif 'davit' in model_name:\n",
        "    model.head.fc = nn.Linear(model.head.fc.in_features, num_classes)\n",
        "  elif 'gcvit' in model_name:\n",
        "    model.head.fc = nn.Linear(model.head.fc.in_features, num_classes)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjOWgYSbhZL2"
      },
      "outputs": [],
      "source": [
        "def init_model(model_name, loss_function='cross_entropy', feature_extract=Config.feature_extract):\n",
        "  model = get_model(model_name)\n",
        "  model = set_fc_layer(model, model_name, loss_function)\n",
        "\n",
        "  if feature_extract:\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'fc' in name:\n",
        "        param.requires_grad = True\n",
        "      elif 'layer4' in name:\n",
        "        param.requires_grad = True\n",
        "      elif 'classifier' in name:\n",
        "        param.requires_grad = True\n",
        "      else:\n",
        "        param.requires_grad = False\n",
        "  else:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {'trainable' if param.requires_grad else 'frozen'}\")\n",
        "\n",
        "  model = model.to(Config.device)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McAy9X_Hr4XN"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BAV9KdhQrpJ"
      },
      "outputs": [],
      "source": [
        "def save_in_google_drive(model_name, file_name):\n",
        "  dest_folder = f\"{root_folder}/{results_folder}/{nr_classes}_classes/{model_name}\"\n",
        "  os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "  if file_name.endswith('.pth'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}/{file_name}\")\n",
        "  elif file_name.endswith('.png'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}/{file_name}\")\n",
        "  elif file_name.endswith('.json'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}{file_name}\")\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid file type: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqBDiKQ1E_F"
      },
      "outputs": [],
      "source": [
        "def plot_and_save_training_curves(model_name, loss_function, train_losses, val_losses, train_accs, val_accs):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(train_losses, label='Erro de treinamento')\n",
        "    ax1.plot(val_losses, label='Erro de validação')\n",
        "    ax1.set_title('Erro de treinamento e validação')\n",
        "    ax1.set_xlabel('Época')\n",
        "    ax1.set_ylabel('Erro')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(train_accs, label='Acurácia de treinamento')\n",
        "    ax2.plot(val_accs, label='Acurácia de validação')\n",
        "    ax2.set_title('Acurácia de treinamento e validação')\n",
        "    ax2.set_xlabel('Época')\n",
        "    ax2.set_ylabel('Acurácia')\n",
        "    ax2.legend()\n",
        "\n",
        "    # save the chart to a file in colab\n",
        "    file_name = f'{model_name}_curves_{loss_function}.png'\n",
        "    plt.savefig(file_name)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew2aqdi-itIS"
      },
      "outputs": [],
      "source": [
        "def save_results(base_file_name, results):\n",
        "  sao_paulo_tz = pytz.timezone('America/Sao_Paulo')\n",
        "  timestamp = datetime.now(sao_paulo_tz).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "  filename = f'{base_file_name}_{timestamp}.json'\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "  save_in_google_drive('', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Aklw4az5alh"
      },
      "outputs": [],
      "source": [
        "def get_model_processor(model_name, model_src):\n",
        "  if model_src == 'pytorch':\n",
        "    return None\n",
        "  if model_name == 'facebook/deit-base-distilled-patch16-224':\n",
        "    return AutoFeatureExtractor.from_pretrained(model_name)\n",
        "  # return ViTImageProcessor.from_pretrained(model_name)\n",
        "  return AutoImageProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLnn-H_sUwVX"
      },
      "outputs": [],
      "source": [
        "def get_results_from_json_file():\n",
        "  file_path = '/content/classification_report_2025-06-20_02-40-18.json'\n",
        "  if not os.path.exists(file_path):\n",
        "    return {}\n",
        "\n",
        "  with open(file_path, 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQRb8sh1wbik"
      },
      "outputs": [],
      "source": [
        "def get_classes_to_exclude():\n",
        "  if nr_classes == 5:\n",
        "    return []\n",
        "  elif nr_classes == 4:\n",
        "    return [1]\n",
        "  elif nr_classes == 3:\n",
        "    return [0,1]\n",
        "  elif nr_classes == 2:\n",
        "    return [0,1,2]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid number of classes: {nr_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGv4JYXd0YNY"
      },
      "outputs": [],
      "source": [
        "def get_cm_labels():\n",
        "  if nr_classes == 5:\n",
        "    return None\n",
        "  elif nr_classes == 4:\n",
        "    return [\"0\", \"2\", \"3\", \"4\"]\n",
        "  elif nr_classes == 3:\n",
        "    return [\"2\", \"3\", \"4\"]\n",
        "  elif nr_classes == 2:\n",
        "    return [\"0\", \"1\"]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid number of classes: {nr_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po4JsPZncz3X"
      },
      "outputs": [],
      "source": [
        "def sanitize_model_name(model_name):\n",
        "  return model_name.replace('/', '-').replace('.', '-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hd21PxGrvSl"
      },
      "outputs": [],
      "source": [
        "def load_model(model, path, map_location='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  print(path)\n",
        "  model.load_state_dict(torch.load(path, map_location=map_location))\n",
        "  model.eval()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0mirvox44Cz"
      },
      "outputs": [],
      "source": [
        "def map_model_labels(y_true, y_pred, labels):\n",
        "  if labels is None:\n",
        "    return y_true, y_pred\n",
        "  return [labels[i] for i in y_true], [labels[i] for i in y_pred]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqZ7ZaTXxqqW"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBXSESKjHq6f"
      },
      "outputs": [],
      "source": [
        "def measure_inference_time(model, dataloader, warmup=5, repeat=50):\n",
        "  model.eval()\n",
        "\n",
        "  inputs, _ = next(iter(dataloader))\n",
        "  inputs = inputs.to(Config.device)\n",
        "\n",
        "  for _ in range(warmup):\n",
        "    with torch.no_grad():\n",
        "      _ = model(inputs)\n",
        "\n",
        "  start = time.time()\n",
        "  for _ in range(repeat):\n",
        "    with torch.no_grad():\n",
        "      _ = model(inputs)\n",
        "  end = time.time()\n",
        "\n",
        "  total_time = end - start\n",
        "  avg_inference_time = total_time / repeat\n",
        "  time_per_sample = avg_inference_time / inputs.shape[0]\n",
        "\n",
        "  print(f\"Inferência média por batch: {avg_inference_time:.6f} segundos\")\n",
        "  print(f\"Inferência média por amostra: {time_per_sample:.6f} segundos\")\n",
        "  return avg_inference_time, time_per_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXEEcchLogVs"
      },
      "outputs": [],
      "source": [
        "def gen_confusion_matrix(model_name, loss_function, y_true, y_pred, labels=None):\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  if labels is None:\n",
        "    labels = 'auto'\n",
        "\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "  plt.title('Matriz de Confusão')\n",
        "  plt.xlabel('Predição')\n",
        "  plt.ylabel('Real')\n",
        "  file_name = f'{model_name}_cm_{loss_function}.png'\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLNwxiAHqHY_"
      },
      "outputs": [],
      "source": [
        "def gen_classification_report(model_name, loss_function, y_true, y_pred, classes_to_exclude):\n",
        "  # calculate overall metrics\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "  report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    # target_names=[f'KL{i}' for i in range(5) if i not in classes_to_exclude],\n",
        "    target_names=[f'{i}' for i in range(5) if i not in classes_to_exclude],\n",
        "    output_dict=True\n",
        "  )\n",
        "  report['kappa'] = kappa\n",
        "  report['qwk'] = qwk\n",
        "  report['mae'] = mae\n",
        "\n",
        "  return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDMPw6NFyzkG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, model_src, model_name, loss_function, test_loader, classes_to_exclude=[], cm_labels=None):\n",
        "  model.eval()\n",
        "  all_preds, all_labels = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  all_preds = np.array(all_preds)\n",
        "  all_labels = np.array(all_labels)\n",
        "\n",
        "  all_labels, all_preds = map_model_labels(all_labels, all_preds, cm_labels)\n",
        "  gen_confusion_matrix(model_name, loss_function, all_labels, all_preds, cm_labels)\n",
        "\n",
        "  return gen_classification_report(model_name, loss_function, all_labels, all_preds, classes_to_exclude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5UxUArOUx-M"
      },
      "outputs": [],
      "source": [
        "def run_model_evaluation():\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  for model_name, model_src in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_src)\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "    model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "    for loss_function in ['cross_entropy', 'corn']:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_name_sanitized)\n",
        "      file_name = f\"{model_name_sanitized}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "      print(path)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "      report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['test'], classes_to_exclude, cm_labels)\n",
        "      print(json.dumps(report, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNcZ2eOtUzPW"
      },
      "source": [
        "### Run evaluation from trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Um_rjJqXxXK"
      },
      "outputs": [],
      "source": [
        "# run_model_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy6rFMkkrlZY"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvtBygmxvHKJ"
      },
      "outputs": [],
      "source": [
        "def print_training_time(start_time, end_time):\n",
        "  total_time = end_time - start_time\n",
        "  print(f\"\\nTraining Time: {total_time / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a2Pu0_khmWd"
      },
      "outputs": [],
      "source": [
        "def compute_loss(outputs, labels, criterion, loss_function):\n",
        "  if loss_function == 'cross_entropy':\n",
        "    return criterion(outputs, labels)\n",
        "  elif loss_function == 'corn':\n",
        "    return criterion(outputs, labels, num_classes=Config.num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbj0-w6Bh1jf"
      },
      "outputs": [],
      "source": [
        "def get_predictions(outputs, loss_function):\n",
        "  if loss_function == 'cross_entropy':\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "  elif loss_function == 'corn':\n",
        "    preds = corn_label_from_logits(outputs)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aDtvKDNdTOE"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, datasize, loss_function, criterion, optimizer, model_name, model_src, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      if model_name == 'inception_v3':\n",
        "        outputs, aux_logits = outputs.logits, outputs.aux_logits\n",
        "        loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "        aux_loss = compute_loss(aux_logits, labels, criterion, loss_function)\n",
        "        loss += 0.4 * aux_loss\n",
        "      else:\n",
        "        loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / datasize\n",
        "  epoch_acc = running_corrects.double() / datasize\n",
        "  return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhtQVhCIigds"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, dataloader, datasize, loss_function, criterion, optimizer, model_src, device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / datasize\n",
        "  epoch_acc = running_corrects.double() / datasize\n",
        "  return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N24lqdciiIMD"
      },
      "outputs": [],
      "source": [
        "def train_model(model_metadata):\n",
        "  # destructuring arguments\n",
        "  model, model_src, model_name = (\n",
        "    model_metadata['model'],\n",
        "    model_metadata['model_src'],\n",
        "    model_metadata['model_name']\n",
        "  )\n",
        "  dataloaders, dataset_sizes = (\n",
        "    model_metadata['dataloaders'],\n",
        "    model_metadata['dataset_sizes']\n",
        "  )\n",
        "  loss_function, criterion, optimizer, scheduler = (\n",
        "    model_metadata['loss_function'],\n",
        "    model_metadata['criterion'],\n",
        "    model_metadata['optimizer'],\n",
        "    model_metadata['scheduler']\n",
        "  )\n",
        "\n",
        "  best_model_wts = model.state_dict()\n",
        "  best_acc = 0.0\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "  early_stopping = EarlyStopping(patience=5, verbose=True, loss_function=loss_function)\n",
        "  start_time = time.time()\n",
        "  print(f\"loss function={loss_function}\")\n",
        "\n",
        "  for epoch in range(1, Config.num_epochs+1):\n",
        "    print(f\"Epoch {epoch}/{Config.num_epochs}\" + \"-\"*10)\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, dataloaders['train'], dataset_sizes['train'], loss_function, criterion, optimizer, model_name, model_src, Config.device)\n",
        "    val_loss, val_acc = validate_one_epoch(model, dataloaders['val'], dataset_sizes['val'], loss_function, criterion, optimizer, model_src, Config.device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "    early_stopping(val_loss, model, model_name)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model_wts = model.state_dict()\n",
        "\n",
        "  end_time = time.time()\n",
        "  print_training_time(start_time, end_time)\n",
        "  print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, train_losses, val_losses, train_accs, val_accs, end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEBqCCg-ktjE"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# def train_model():\n",
        "#   # destructuring arguments\n",
        "#   # model, model_src, model_name = (\n",
        "#   #   model_data['model'],\n",
        "#   #   model_data['model_src'],\n",
        "#   #   model_data['model_name']\n",
        "#   # )\n",
        "#   # dataloaders, dataset_sizes = (\n",
        "#   #   model_data['dataloaders'],\n",
        "#   #   model_data['dataset_sizes']\n",
        "#   # )\n",
        "#   # loss_function, criterion, optimizer, scheduler = (\n",
        "#   #   model_data['loss_function'],\n",
        "#   #   model_data['criterion'],\n",
        "#   #   model_data['optimizer'],\n",
        "#   #   model_data['scheduler']\n",
        "#   # )\n",
        "#   # num_epochs = model_data['num_epochs']\n",
        "\n",
        "#   best_model_wts = model.state_dict()\n",
        "#   best_acc = 0.0\n",
        "#   train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "#   early_stopping = EarlyStopping(patience=5, verbose=True, loss_function=loss_function)\n",
        "#   start_time = time.time()\n",
        "\n",
        "#   for epoch in range(1, Config.num_epochs+1):\n",
        "#     print(f\"Epoch {epoch}/{Config.num_epochs}\" + \"-\"*10)\n",
        "\n",
        "#     for phase in ['train', 'val']:\n",
        "#       if phase == 'train':\n",
        "#         model.train()\n",
        "#       else:\n",
        "#         model.eval()\n",
        "\n",
        "#       running_loss = 0.0\n",
        "#       running_corrects = 0\n",
        "\n",
        "#       for inputs, labels in tqdm(dataloaders[phase]):\n",
        "#         inputs = inputs.to(Config.device)\n",
        "#         labels = labels.to(Config.device)\n",
        "\n",
        "#         # clear the gradients of all optimized parameters\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         with torch.set_grad_enabled(phase == 'train'):\n",
        "#           # forward pass: obtain the model predictions for the input data\n",
        "#           outputs = model(inputs)\n",
        "#           outputs = outputs.logits if model_src == 'hugging_face' else outputs\n",
        "\n",
        "#           if model_name == 'inception_v3' and phase == 'train':\n",
        "#             _, preds = torch.max(outputs.logits, 1) if loss_function == 'cross_entropy' else (None, corn_label_from_logits(outputs.logits))\n",
        "\n",
        "#             # compute the loss between the model predictions and the true labels\n",
        "#             loss = criterion(outputs.logits, labels) if loss_function == 'cross_entropy' else criterion(outputs.logits, labels, num_classes=Config.num_classes)\n",
        "#           else:\n",
        "#             _, preds = torch.max(outputs, 1) if loss_function == 'cross_entropy' else (None, corn_label_from_logits(outputs))\n",
        "\n",
        "#             # compute the loss between the model predictions and the true labels\n",
        "#             loss = criterion(outputs, labels) if loss_function == 'cross_entropy' else criterion(outputs, labels, num_classes=Config.num_classes)\n",
        "\n",
        "#           if phase == 'train':\n",
        "#             if model_name == 'inception_v3':\n",
        "#               aux_logits = outputs.aux_logits\n",
        "#               aux_loss = criterion(aux_logits, labels) if loss_function == 'cross_entropy' else criterion(aux_logits, labels, num_classes=Config.num_classes)\n",
        "#               loss += 0.4 * aux_loss\n",
        "\n",
        "#             # backward pass: compute gradients of the loss with respect to model parameters\n",
        "#             loss.backward()\n",
        "\n",
        "#             # update the model parameters using the computed gradients\n",
        "#             optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#       epoch_loss = running_loss / dataset_sizes[phase]\n",
        "#       epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "#       if phase == 'train':\n",
        "#         scheduler.step()\n",
        "#         train_losses.append(epoch_loss)\n",
        "#         train_accs.append(epoch_acc.item())\n",
        "#       else:\n",
        "#         val_losses.append(epoch_loss)\n",
        "#         val_accs.append(epoch_acc.item())\n",
        "\n",
        "#       print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "#       # Early stopping\n",
        "#       if phase == 'val':\n",
        "#         early_stopping(epoch_loss, model, model_name)\n",
        "#         if early_stopping.early_stop:\n",
        "#           print(\"Early stopping\")\n",
        "#           end_time = time.time()\n",
        "#           total_time = end_time - start_time\n",
        "#           print_training_time(start_time, end_time)\n",
        "#           print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "#           model.load_state_dict(best_model_wts)\n",
        "#           return model, train_losses, val_losses, train_accs, val_accs, total_time\n",
        "\n",
        "#       if phase == 'val' and epoch_acc > best_acc:\n",
        "#         best_acc = epoch_acc\n",
        "#         best_model_wts = model.state_dict()\n",
        "\n",
        "#     print(\"\")\n",
        "\n",
        "#   # record the end time\n",
        "#   end_time = time.time()\n",
        "#   total_time = end_time - start_time\n",
        "#   print_training_time(start_time, end_time)\n",
        "\n",
        "#   print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "#   model.load_state_dict(best_model_wts)\n",
        "\n",
        "#   return model, train_losses, val_losses, train_accs, val_accs, total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlNhv3x2tS3H"
      },
      "source": [
        "### Model's list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZVcpzi4w26Q"
      },
      "outputs": [],
      "source": [
        "models_list = {\n",
        "    'resnet34': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet34',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'resnet50': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet50',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'resnet101': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet101',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'vgg16': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'vgg16',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'vgg19': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'vgg19',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'densenet121': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'densenet121',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'densenet169': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'densenet169',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'inception_v3': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'inception_v3',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'facebook/deit-base-distilled-patch16-224': {\n",
        "        'type': 'vit',\n",
        "        'source': 'hugging_face',\n",
        "        'shortname': 'facebook-deit-base-distilled-patch16-224',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'davit_base.msft_in1k': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'davit_base-msft_in1k',\n",
        "        'skip': False,\n",
        "    },\n",
        "    'maxvit_t': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'maxvit_t',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'gcvit_base.in1k': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'gcvit_base-in1k',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'swin_b': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'swin_b',\n",
        "        'skip': True,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r6XnjfE02Ba"
      },
      "outputs": [],
      "source": [
        "loss_functions = ['cross_entropy', 'corn']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ptFfpa4nuk"
      },
      "outputs": [],
      "source": [
        "def run_training():\n",
        "  results = get_results_from_json_file()\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  for model_name, model_src in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_src)\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "    model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "    results[model_name_sanitized] = {}\n",
        "\n",
        "    for loss_function in ['cross_entropy', 'corn']:\n",
        "      model = init_model(model_name, loss_function)\n",
        "      criterion = nn.CrossEntropyLoss() if loss_function == 'cross_entropy' else corn_loss\n",
        "      optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "      scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "      # defining model metadata dict\n",
        "      model_metadata = {\n",
        "          'model': model,\n",
        "          'model_src': model_src,\n",
        "          'model_name': model_name_sanitized,\n",
        "          'dataloaders': dataloaders,\n",
        "          'dataset_sizes': dataset_sizes,\n",
        "          'loss_function': loss_function,\n",
        "          'criterion': criterion,\n",
        "          'optimizer': optimizer,\n",
        "          'scheduler': scheduler\n",
        "      }\n",
        "\n",
        "      # train the model using transfer learning\n",
        "      model, train_losses, val_losses, train_accs, val_accs, train_time = train_model(model_metadata)\n",
        "\n",
        "      save_in_google_drive(model_name_sanitized, f'{model_name_sanitized}_{loss_function}.pth')\n",
        "      plot_and_save_training_curves(model_name_sanitized, loss_function, train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "      # evaluate the model using test dataloader\n",
        "      report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['test'], classes_to_exclude, cm_labels)\n",
        "\n",
        "      # get model complexity info\n",
        "      flops, params = get_model_complexity_info(model, (3, 224, 224), as_strings=True, backend='pytorch', print_per_layer_stat=False)\n",
        "\n",
        "      report['train_time'] = train_time\n",
        "      report['flops'] = flops\n",
        "      report['params'] = params\n",
        "\n",
        "      results[model_name_sanitized][loss_function] = report\n",
        "\n",
        "  # save report\n",
        "  save_results('classification_report', results)\n",
        "  print(f'Training completed. Reports saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owtVL4sXXKnU"
      },
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqB3rYOkVY8z"
      },
      "outputs": [],
      "source": [
        "# run_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rO4kF4b7nbp"
      },
      "source": [
        "# Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAIvUN4G7qZo"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  results = get_results_from_json_file()\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  # === Suggest hyperparameters with Optuna ===\n",
        "  model_name = trial.suggest_categorical(\"model_name\", [\"densenet169\", \"davit_base.msft_in1k\", \"densenet121\", \"gcvit_base.in1k\", \"maxvit_t\"])\n",
        "  loss_function = trial.suggest_categorical(\"loss_function\", [\"cross_entropy\", \"corn\"])\n",
        "  learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "  feature_extract = trial.suggest_categorical(\"feature_extract\", [True, False])\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [16, 28, 32, 64])\n",
        "\n",
        "  model_src = \"pytorch\"\n",
        "  model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "  # === Init model ===\n",
        "  processor = get_model_processor(model_name, model_src)\n",
        "  dataloaders, dataset_sizes = load_dataset(model_name, processor, batch_size=batch_size)\n",
        "  model = init_model(model_name, loss_function, feature_extract)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss() if loss_function == 'cross_entropy' else corn_loss\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "  model_metadata = {\n",
        "    'model': model,\n",
        "    'model_src': model_src,\n",
        "    'model_name': model_name_sanitized,\n",
        "    'dataloaders': dataloaders,\n",
        "    'dataset_sizes': dataset_sizes,\n",
        "    'loss_function': loss_function,\n",
        "    'criterion': criterion,\n",
        "    'optimizer': optimizer,\n",
        "    'scheduler': scheduler\n",
        "  }\n",
        "\n",
        "  # === Training ===\n",
        "  model, train_losses, val_losses, train_accs, val_accs, train_time = train_model(model_metadata)\n",
        "\n",
        "  # === Evaluate ===\n",
        "  report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['val'], classes_to_exclude, cm_labels)\n",
        "\n",
        "  # === Free GPU\n",
        "  del model\n",
        "  del dataloaders\n",
        "  del model_metadata\n",
        "  gc2.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Returning the metric to maximize\n",
        "  return report['qwk']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1ptu97CjWG"
      },
      "source": [
        "## Run optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDwll1xACtPS"
      },
      "outputs": [],
      "source": [
        "def run_optuna():\n",
        "  study = optuna.create_study(direction=\"maximize\")\n",
        "  study.optimize(objective, n_trials=30, timeout=36000) # 10h\n",
        "\n",
        "  print(\"Melhores hiperparâmetros encontrados:\")\n",
        "  print(study.best_params)\n",
        "\n",
        "  # (Opcional) salvar\n",
        "  joblib.dump(study, \"optuna_study.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3g9itljDEed"
      },
      "outputs": [],
      "source": [
        "# run_optuna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLTydxfrtF-"
      },
      "source": [
        "# Conformal prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJzK33tz4ygr"
      },
      "source": [
        "### Calculate non-conformity scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSGOMGcokXd6"
      },
      "outputs": [],
      "source": [
        "def xe_calc_conformity_score(logits, y):\n",
        "  probs = F.softmax(logits, dim=0)\n",
        "  sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "  score = 0.0\n",
        "\n",
        "  for k, idx in enumerate(sorted_indices):\n",
        "    if idx.item() == y:\n",
        "      score = sorted_probs[:k+1].sum().item()\n",
        "      return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwv9qw2GBaP0"
      },
      "outputs": [],
      "source": [
        "def corn_calc_conformity_score(logits, y):\n",
        "  probs = torch.sigmoid(logits)\n",
        "  score = 0.0\n",
        "\n",
        "  for k in range(nr_classes - 1):\n",
        "    p = probs[k].item()\n",
        "    if k >= y:\n",
        "      score += p\n",
        "    else:\n",
        "      score += 1 - p\n",
        "\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYB1PsYWuCDw"
      },
      "outputs": [],
      "source": [
        "def calc_nonconformity_scores(model, model_src, calib_loader, loss_function):\n",
        "  model.eval()\n",
        "  scores = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in tqdm(calib_loader):\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits # shape: (B, K-1) para CORN\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        y = labels[i].item()\n",
        "        logits_i = outputs[i]\n",
        "\n",
        "        if loss_function == 'corn':\n",
        "          scores.append(corn_calc_conformity_score(logits_i, y))\n",
        "        else:\n",
        "          scores.append(xe_calc_conformity_score(logits_i, y))\n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt_f4yNu3zPq"
      },
      "outputs": [],
      "source": [
        "# corn\n",
        "def calc_per_threshold_scores(model, model_src, calib_loader):\n",
        "  model.eval()\n",
        "  per_threshold_scores = [[] for _ in range(nr_classes - 1)] # K-1 thresholds\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in tqdm(calib_loader):\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits # shape: (B, K-1)\n",
        "\n",
        "      probs = torch.sigmoid(outputs)\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        y = labels[i].item()\n",
        "        for k in range(nr_classes - 1):\n",
        "          p = probs[i, k].item()\n",
        "          if y > k:\n",
        "            score = 1 - p\n",
        "          else:\n",
        "            score = p\n",
        "          per_threshold_scores[k].append(score)\n",
        "\n",
        "  return per_threshold_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHpVD9E34uvp"
      },
      "outputs": [],
      "source": [
        "def get_prediction_set(probs, q_hat):\n",
        "  sorted_indices = torch.argsort(probs, descending=True)\n",
        "  pred_set = []\n",
        "  cumulative = 0\n",
        "\n",
        "  for idx in sorted_indices:\n",
        "    pred_set.append(idx.item())\n",
        "    cumulative += probs[idx].item()\n",
        "    if cumulative >= q_hat:\n",
        "      break\n",
        "  return pred_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ8uxFYO5kUU"
      },
      "outputs": [],
      "source": [
        "def get_prediction_interval(logits, q_hat):\n",
        "  probs = torch.sigmoid(logits) # shape: (K-1,)\n",
        "  score = 0.0\n",
        "  upper_class = len(probs)\n",
        "\n",
        "  for k in range(len(probs)):\n",
        "    score += 1 - probs[k].item()\n",
        "    if score > q_hat:\n",
        "      upper_class = k\n",
        "      break\n",
        "\n",
        "  return list(range(upper_class + 1))  # intervalo ordinal: [0, ..., upper_class]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kie4xsW75sHJ"
      },
      "outputs": [],
      "source": [
        "def predict_interval_from_thresholds(logits, thresholds):\n",
        "  probs = torch.sigmoid(logits)\n",
        "  interval = []\n",
        "\n",
        "  for k, tau_k in enumerate(thresholds):\n",
        "    if 1 - probs[k].item() <= tau_k:\n",
        "        interval.append(k + 1)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "  return list(range(interval[-1] + 1)) if interval else [0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1HVivB3a5Yd"
      },
      "outputs": [],
      "source": [
        "def run_conformal_prediction(dataloader, model, model_src, thresholds, loss_function):\n",
        "  model.eval()\n",
        "  set_sizes = []\n",
        "  coverage = 0.0\n",
        "  total = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        if loss_function == 'corn':\n",
        "          # pred_set = get_prediction_interval(outputs[i], q_hat)\n",
        "          pred_set = predict_interval_from_thresholds(outputs[i], thresholds)\n",
        "        else:\n",
        "          probs = F.softmax(outputs, dim=1)\n",
        "          pred_set = get_prediction_set(probs[i], thresholds[0])\n",
        "\n",
        "        true_label = labels[i].item()\n",
        "        print(f\"Predicted set: {pred_set}, True label: {true_label}\")\n",
        "        set_sizes.append(len(pred_set))\n",
        "        coverage += 1 if true_label in pred_set else 0\n",
        "        total += 1\n",
        "\n",
        "  coverage /= total\n",
        "  return set_sizes, coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DFu96qtKpQ0"
      },
      "outputs": [],
      "source": [
        "def plot_and_save_set_sizes(set_sizes, model_name, loss_function):\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.hist(set_sizes, bins=np.arange(min(set_sizes), max(set_sizes) + 2))\n",
        "  plt.xlabel('Tamanho do conjunto de predições')\n",
        "  plt.ylabel('Frequência')\n",
        "  plt.title('Distribuição do tamanho do conjunto de predições')\n",
        "\n",
        "  file_name = f'{model_name}_cp_{loss_function}.png'\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay8fWPfQ5Sjn"
      },
      "outputs": [],
      "source": [
        "def compute_thresholds(per_threshold_scores, epsilon):\n",
        "  thresholds = []\n",
        "  for scores_k in per_threshold_scores:\n",
        "    n = len(scores_k)\n",
        "    q_hat = np.quantile(scores_k, math.ceil((n+1)*(1-epsilon))/n)\n",
        "    thresholds.append(q_hat)\n",
        "  return thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIUNRT1sPP2x"
      },
      "outputs": [],
      "source": [
        "def squash_set_sizes(set_sizes):\n",
        "  squashed_set_sizes = [0 for _ in range(nr_classes+1)]\n",
        "  for size in set_sizes:\n",
        "    squashed_set_sizes[size] += 1\n",
        "  return squashed_set_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0lr5dJDJ3Ws"
      },
      "outputs": [],
      "source": [
        "def conformal_prediction():\n",
        "  results = {}\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    results[model_attr['shortname']] = {}\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      print(f\"model name: {model_attr['shortname']} - loss function: {loss_function}\")\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      thresholds = None\n",
        "\n",
        "      # calculating the non-confirmity scores\n",
        "      if loss_function == 'corn':\n",
        "        per_threshold_scores = calc_per_threshold_scores(model, model_attr['source'], dataloaders['calib'])\n",
        "        thresholds = compute_thresholds(per_threshold_scores, Config.epsilon)\n",
        "      else:\n",
        "        scores = calc_nonconformity_scores(model, model_attr['source'], dataloaders['calib'], loss_function)\n",
        "        thresholds = compute_thresholds([scores], Config.epsilon)\n",
        "\n",
        "      set_sizes, coverage = run_conformal_prediction(dataloaders['test'], model, model_attr['source'], thresholds, loss_function)\n",
        "\n",
        "      plot_and_save_set_sizes(set_sizes, model_attr['shortname'], loss_function)\n",
        "\n",
        "      results[model_attr['shortname']][loss_function] = {\n",
        "        'q_hat': thresholds,\n",
        "        'coverage': coverage,\n",
        "        'set_sizes': squash_set_sizes(set_sizes)\n",
        "      }\n",
        "\n",
        "  save_results('conformal_prediction', results)\n",
        "  print(f'Conformal prediction completed. Reports saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSmRX7kJx1tg"
      },
      "source": [
        "### Run conformal prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5apgv2_vYA1n"
      },
      "outputs": [],
      "source": [
        "conformal_prediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZkNjSezL4OQ"
      },
      "source": [
        "# Inference Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWQqKtj7L-6g"
      },
      "outputs": [],
      "source": [
        "def run_model_inference_time():\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "  results = {}\n",
        "\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    results[model_attr['shortname']] = {}\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      avg_inference_time, time_per_sample = measure_inference_time(model, dataloaders['test'], warmup=5, repeat=50)\n",
        "\n",
        "      results[model_attr['shortname']][loss_function] = {\n",
        "          'avg_inference_time': avg_inference_time,\n",
        "          'time_per_sample': time_per_sample\n",
        "      }\n",
        "\n",
        "  save_results('inference_time', results)\n",
        "  print(f'Inference time completed. Reports saved.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhQ_ug3KMoMl"
      },
      "outputs": [],
      "source": [
        "# run_model_inference_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT7RzUMF7Al7"
      },
      "source": [
        "# Visualizing with Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dciC0HUGwGe6"
      },
      "outputs": [],
      "source": [
        "grad_cam_criteria = {\n",
        "    'per-kl-criteria': 'per_kl_class'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQAoVfeVP-ZX"
      },
      "outputs": [],
      "source": [
        "class HuggingFaceWrapper(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x).logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvkBmfxbALrK"
      },
      "outputs": [],
      "source": [
        "class CORNWrapper(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model  # base model returning CORN logits\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)  # logits of shape [B, K-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWnBlwJrwpP3"
      },
      "outputs": [],
      "source": [
        "class CORNOutputTarget:\n",
        "  def __init__(self, class_index, epsilon=1e-1):\n",
        "    self.class_index = class_index  # 0-based class index\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def __call__(self, model_output):\n",
        "    \"\"\"\n",
        "    CORN produces K-1 outputs.\n",
        "    We define a differentiable proxy score that encourages correct attribution.\n",
        "    \"\"\"\n",
        "    # Apply sigmoid to logits: shape [K-1]\n",
        "    print(model_output)\n",
        "    probs = torch.sigmoid(model_output)\n",
        "    print(\"probs:\", probs)\n",
        "    print(\"prob shape:\", probs.shape)\n",
        "\n",
        "    # Calculate pseudo-probability for class_index\n",
        "    # For class 0: P(class = 0) = 1 - sigmoid(logit_0)\n",
        "    # For class k: P(class = k) = sigmoid(logit_{k-1}) - sigmoid(logit_{k})\n",
        "    # if self.class_index == 0:\n",
        "    #     score = 1 - probs[0]\n",
        "    # elif self.class_index == probs.shape[0]:\n",
        "    #     score = probs[-1]\n",
        "    # else:\n",
        "    #     score = probs[self.class_index - 1] - probs[self.class_index]\n",
        "\n",
        "    if self.class_index == 0:\n",
        "      return 1 - probs[0]\n",
        "\n",
        "    return probs[:self.class_index].sum()\n",
        "    # return probs[:, self.class_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mO9GXNFnIfZ"
      },
      "outputs": [],
      "source": [
        "def resize_image(image, model_name, processor):\n",
        "  if not processor:\n",
        "    image_size = (224, 224) if (model_name != 'inception_v3') else (299, 299)\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    image_size = (processor.crop_size['height'], processor.crop_size['height'])\n",
        "  else:\n",
        "    image_size = (processor.size['height'], processor.size['height'])\n",
        "  return image.resize(image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ3cG1eoxfI3"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "  images = []\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.png'):\n",
        "      image_path = os.path.join(folder_path, filename)\n",
        "      images.append(image_path)\n",
        "  return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_PiXR7msDrR"
      },
      "outputs": [],
      "source": [
        "def image_to_tensor(img_path, model_name, processor):\n",
        "  img_pil = Image.open(img_path).convert('RGB')\n",
        "  image_resized = resize_image(img_pil, model_name, processor)\n",
        "  transform = get_transforms(model_name, 'test')\n",
        "  img_tensor = transform(image_resized).unsqueeze(0)\n",
        "  img_np = np.array(image_resized).astype(np.float32)/255.0\n",
        "  return img_tensor, img_np, img_pil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PLi8cj81S7T"
      },
      "outputs": [],
      "source": [
        "def get_target_layer(model, model_name):\n",
        "  if model_name in ['resnet34', 'resnet50', 'resnet101']:\n",
        "    return [model.layer4[-1]]\n",
        "  elif model_name in ['vgg16', 'vgg19']:\n",
        "    return [model.features[-1]]\n",
        "  elif model_name in ['densenet121', 'densenet169']:\n",
        "    return [model.features[-1]]\n",
        "  elif model_name in ['inception_v3']:\n",
        "    return [model.Mixed_7c]\n",
        "  elif model_name in ['google/vit-base-patch16-224']:\n",
        "    return [model.model.vit.encoder.layer[-1].layernorm_before]\n",
        "  elif model_name in ['facebook/deit-base-distilled-patch16-224']:\n",
        "    return [model.model.deit.encoder.layer[-1].layernorm_before]\n",
        "  elif model_name in ['davit_base.msft_in1k']:\n",
        "    return [model.stages[3].blocks[0][1].norm1]\n",
        "  elif model_name in ['maxvit_t']:\n",
        "    return [model.blocks[-1].layers[-1].layers[-1].attn_layer[0]]\n",
        "  elif model_name in ['gcvit_base.in1k']:\n",
        "    return [model.stages[-1].blocks[-1].norm2]\n",
        "  elif model_name in ['swin_b']:\n",
        "    return [model.features[-1][-1].norm1]\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6WPXnn5zFiU"
      },
      "outputs": [],
      "source": [
        "def reshape_transform(tensor):\n",
        "  # tensor shape: [batch_size, seq_len, hidden_dim]\n",
        "  # remove CLS token and reshape to 2D feature map\n",
        "\n",
        "  print(tensor.size())\n",
        "  # tensor: [B, H, N, C]\n",
        "  if tensor.dim() == 4:\n",
        "    B, H, W, C = tensor.shape\n",
        "    if H == 1: # maxvit_t\n",
        "      tensor = tensor.squeeze(1)\n",
        "    else:\n",
        "      return tensor.permute(0, 3, 1, 2)\n",
        "\n",
        "  B, seq_len, hidden_dim = tensor.size()\n",
        "\n",
        "  if seq_len == 197: # stardard vit with CLS\n",
        "    tensor = tensor[:, 1:, :]\n",
        "  elif seq_len == 198: # deit-distilled\n",
        "    tensor = tensor[:, 2:, :]\n",
        "  elif seq_len == 49: # davit, and others\n",
        "    pass # does not prepend CLS by default\n",
        "  elif seq_len == 77: # maxvit\n",
        "    tensor = tensor[:, 1:, :]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid sequence length: {seq_len}\")\n",
        "\n",
        "  h = w = int(tensor.shape[1] ** 0.5)\n",
        "  if h * w != tensor.shape[1]:\n",
        "      raise ValueError(f\"Cannot reshape: {tensor.shape[1]} tokens is not a square\")\n",
        "\n",
        "  # reshape to [batch, hidden_dim, height, width]\n",
        "  return tensor.permute(0, 2, 1).reshape(B, hidden_dim, h, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNDbRxvySki9"
      },
      "outputs": [],
      "source": [
        "def reshape_transform_swinb(tensor, height=7, width=7):\n",
        "  result = tensor[:, 1:,:].reshape(tensor.size(0), height, width, tensor.size(2))\n",
        "\n",
        "  # Bring the channels to the first dimension,\n",
        "  # like in CNNs.\n",
        "  result = result.transpose(2, 3).transpose(1, 2)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C57y5atXUk2Z"
      },
      "outputs": [],
      "source": [
        "def run_gradcam(model, model_attr, loss_function, target_layers, input_tensor, img_np, file_name):\n",
        "  input_tensor = input_tensor.to(Config.device)\n",
        "  input_tensor.requires_grad = True\n",
        "\n",
        "  model.eval()\n",
        "  model.zero_grad()\n",
        "\n",
        "  if model_attr.get('shortname') == 'swin_bo':\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers,\n",
        "                  reshape_transform=reshape_transform_swinb)\n",
        "  elif model_attr.get('type') == 'vit':\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers,\n",
        "                  reshape_transform=reshape_transform)\n",
        "  else:\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers)\n",
        "\n",
        "  if type(model) == HuggingFaceWrapper:\n",
        "    outputs = model.model(input_tensor).logits\n",
        "  else:\n",
        "    outputs = model(input_tensor)\n",
        "\n",
        "  pred_class = get_predictions(outputs, loss_function).item()\n",
        "  print(f\"Predicted class: {pred_class}\")\n",
        "\n",
        "  if loss_function == 'corn':\n",
        "    targets = [CORNOutputTarget(pred_class)]\n",
        "  else:\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "  grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
        "  cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "  plt.imshow(cam_image)\n",
        "  plt.axis('off')\n",
        "  plt.title(f'Escala KL predita: {pred_class}')\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_attr.get('shortname'), file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mQgIQHGzBkF"
      },
      "outputs": [],
      "source": [
        "def run_batch_gradcam(criteria_name, folder_path, model_name, model_attr, loss_function):\n",
        "  processor = get_model_processor(model_name, model_attr.get('source'))\n",
        "\n",
        "  image_paths = load_images_from_folder(folder_path)\n",
        "  if len(image_paths) == 0:\n",
        "    print(f\"Nenhuma imagem encontrada na pasta: {folder_path}\")\n",
        "    return\n",
        "\n",
        "  model = init_model(model_name, loss_function)\n",
        "  base_path = get_results_dir(nr_classes, model_attr.get('shortname'))\n",
        "  file_name = f\"{model_attr.get('shortname')}_{loss_function}.pth\"\n",
        "  path = os.path.join(base_path, file_name)\n",
        "  model = load_model(model, path)\n",
        "\n",
        "  if model_attr.get('source') == 'hugging_face':\n",
        "    model = HuggingFaceWrapper(model)\n",
        "\n",
        "  print(model)\n",
        "\n",
        "  target_layers = get_target_layer(model, model_name)\n",
        "  print(target_layers)\n",
        "\n",
        "  if target_layers is None:\n",
        "    print(f\"Nenhuma camada de destino encontrada para o modelo: {model_name}\")\n",
        "    return\n",
        "\n",
        "  for img_path in image_paths:\n",
        "    input_tensor, img_np, _ = image_to_tensor(img_path, model_name, processor)\n",
        "    file_name = f\"{criteria_name}_gradcam_loss_{loss_function}_{os.path.basename(img_path)}\"\n",
        "    run_gradcam(model, model_attr, loss_function, target_layers, input_tensor, img_np, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkmbpC_DdQk-"
      },
      "source": [
        "### Run Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7zi1LkgyNk4"
      },
      "outputs": [],
      "source": [
        "def exec_grad_cam():\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "    for loss_function in loss_functions:\n",
        "      for criteria_name, criteria_path in grad_cam_criteria.items():\n",
        "        folder_path = get_grad_cam_dir(Config.num_classes, criteria_path)\n",
        "        run_batch_gradcam(criteria_name, folder_path, model_name, model_attr, loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B49KJTfT4ZAJ"
      },
      "outputs": [],
      "source": [
        "# exec_grad_cam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlGIVKV6HWRF"
      },
      "outputs": [],
      "source": [
        "# gc2.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKalIcmVDR0y"
      },
      "source": [
        "# AUC-ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDEIkR-DVGhK"
      },
      "outputs": [],
      "source": [
        "def corn_probs(logits: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Converte logits do modelo treinado com CORN para distribuição de probabilidade por classe.\n",
        "  Supondo K classes, logits tem shape [B, K-1].\n",
        "  \"\"\"\n",
        "  prob_gt = torch.sigmoid(logits)\n",
        "  prob_le = 1.0 - prob_gt          # P(y ≤ k)\n",
        "\n",
        "  B, K_minus_1 = prob_gt.shape\n",
        "  K = K_minus_1 + 1\n",
        "\n",
        "  # Inicializa tensor de probabilidades por classe\n",
        "  probs = torch.zeros((B, K), device=logits.device)\n",
        "\n",
        "  # Primeira classe: P(y == 0) = 1 - P(y > 0)\n",
        "  probs[:, 0] = prob_le[:, 0]\n",
        "\n",
        "  # Classes intermediárias: P(y == k) = P(y > k-1) * (1 - P(y > k))\n",
        "  for k in range(1, K - 1):\n",
        "      probs[:, k] = prob_gt[:, k - 1] * prob_le[:, k]\n",
        "\n",
        "  # Última classe: P(y == K-1) = P(y > K-2)\n",
        "  probs[:, -1] = prob_gt[:, -1]\n",
        "\n",
        "  return probs  # shape [B, K]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6RNYhNhQZx9"
      },
      "outputs": [],
      "source": [
        "def plot_auc_roc(model, model_attr, test_loader, loss_function):\n",
        "  \"\"\"\n",
        "  Compute and plot the AUC-ROC curve for a PyTorch image classification model.\n",
        "\n",
        "  Parameters:\n",
        "  - model: Trained PyTorch model.\n",
        "  - dataloader: DataLoader with test/validation dataset.\n",
        "  - device: 'cuda' or 'cpu'.\n",
        "  - num_classes: Number of classes (e.g., 5 for KL grades 0-4).\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  model.to(Config.device)\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_attr['source'] == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      if loss_function == 'corn':\n",
        "          preds = corn_probs(outputs)\n",
        "      else:\n",
        "        preds = F.softmax(outputs, dim=1)\n",
        "\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  all_preds = np.array(all_preds)\n",
        "  all_labels = np.array(all_labels)\n",
        "\n",
        "  # Stack all predictions and targets\n",
        "  y_scores = np.vstack(all_preds)\n",
        "  y_true = np.hstack(all_labels)\n",
        "\n",
        "  # Plot ROC curve for each class\n",
        "  plt.figure(figsize=(8, 6))\n",
        "\n",
        "  if Config.num_classes == 2:\n",
        "    # Binary classification\n",
        "    if y_scores.shape[1] == 1:\n",
        "      y_scores = y_scores[:, 0]\n",
        "    else:\n",
        "      y_scores = y_scores[:, 1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"(AUC = {roc_auc:.2f})\")\n",
        "  else:\n",
        "    # Multiclass case\n",
        "    assert y_scores.shape == (len(y_true), Config.num_classes), \\\n",
        "        f\"Expected y_scores shape ({len(y_true)}, {Config.num_classes}), got {y_scores.shape}\"\n",
        "\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(Config.num_classes))\n",
        "    assert y_true_bin.shape == y_scores.shape, \\\n",
        "        f\"Shape mismatch: y_true_bin {y_true_bin.shape}, y_scores {y_scores.shape}\"\n",
        "\n",
        "    for i in range(Config.num_classes):\n",
        "      fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      plt.plot(fpr, tpr, lw=2, label=f\"KL {i} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "  # Random classifier line\n",
        "  plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Aleatório\")\n",
        "\n",
        "  plt.xlabel(\"FPR\")\n",
        "  plt.ylabel(\"TPR\")\n",
        "  plt.title(\"Curva AUC-ROC\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  file_name = f\"{model_attr['shortname']}_auc_roc_{loss_function}.png\"\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_attr['shortname'], file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTK8cQmGERxN"
      },
      "outputs": [],
      "source": [
        "def run_auc_roc():\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      plot_auc_roc(model, model_attr, dataloaders['test'], loss_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-h7tj5jObGp"
      },
      "source": [
        "## Run AUC-ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_cqcDo6LGja"
      },
      "outputs": [],
      "source": [
        "# run_auc_roc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92mNgiuaOdXb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8ZkNjSezL4OQ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
