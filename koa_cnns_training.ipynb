{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83cR6_gZbYYH"
      },
      "source": [
        "## Installing and importing necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLsXK1Qq6VA"
      },
      "source": [
        "### CORN loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3j_wVnlILUM",
        "outputId": "d79f8f80-63a4-4a37-bd97-01bdf1de4b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting coral-pytorch\n",
            "  Downloading coral_pytorch-1.4.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from coral-pytorch) (75.2.0)\n",
            "Downloading coral_pytorch-1.4.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Installing collected packages: coral-pytorch\n",
            "Successfully installed coral-pytorch-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install coral-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzicEbjuq9bb"
      },
      "source": [
        "### GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YiDPJJp6dw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3492a8-0381-4ba5-96d4-ba1e3ab2d379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.21.0+cu124)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=cecd7d47b3e1ed4a7c9e5d2b60947961f04e96f80e0cdee2865ab85c4275604f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/52/78/893c3b94279ef238f43a9e89608af648de401b96415bebbd1f\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, grad-cam\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed grad-cam-1.5.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 ttach-0.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install grad-cam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_vjxMwtq_4l"
      },
      "source": [
        "### Pytorch flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Uh9PZOC6Iyw",
        "outputId": "91b3561f-9c0f-4d65-8e51-cd07436af3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ptflops\n",
            "  Downloading ptflops-0.7.4-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\n",
            "Downloading ptflops-0.7.4-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ptflops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna"
      ],
      "metadata": {
        "id": "omjxv4T8EZ0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KNO4WkMEbbo",
        "outputId": "5b7956ee-7f99-480d-86bb-a7a52ba1d933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.42)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4xztqHerHTY"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGgEhzz7abp0"
      },
      "outputs": [],
      "source": [
        "# basic libs\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import json\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "# pytorch libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from ptflops import get_model_complexity_info\n",
        "from transformers import (ViTForImageClassification,\n",
        "                          ViTImageProcessor,\n",
        "                          DeiTForImageClassification,\n",
        "                          DeiTForImageClassificationWithTeacher,\n",
        "                          DeiTImageProcessor,\n",
        "                          AutoFeatureExtractor,\n",
        "                          AutoImageProcessor,\n",
        "                          AutoModelForImageClassification)\n",
        "\n",
        "# coral pytorch\n",
        "from coral_pytorch.losses import corn_loss\n",
        "from coral_pytorch.dataset import corn_label_from_logits\n",
        "\n",
        "# pytorch grad cam\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# plotting libs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             cohen_kappa_score, mean_absolute_error, confusion_matrix, classification_report,\n",
        "                             roc_curve, auc)\n",
        "from sklearn.preprocessing import label_binarize, OneHotEncoder\n",
        "import gc as gc2\n",
        "import timm\n",
        "\n",
        "import optuna\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a-vV3hXlMFH"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UMjqRzSlN76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7eebd6-2040-4d4e-f6b4-0df056d03efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdd6lk6BmIVn"
      },
      "outputs": [],
      "source": [
        "root_folder = '/content/drive/MyDrive/pgc'\n",
        "dataset_prefix = 'preprocessed_dataset'\n",
        "results_folder = 'trained_models'\n",
        "grad_cam_folder = 'grad_cam'\n",
        "nr_classes = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbsBVB1rPoEy"
      },
      "outputs": [],
      "source": [
        "def get_dataset_dir(nr_classes):\n",
        "  return f'{root_folder}/{dataset_prefix}_{nr_classes}_classes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uMz1S0BuMek"
      },
      "outputs": [],
      "source": [
        "def get_results_dir(nr_classes, model_name):\n",
        "  return f'{root_folder}/{results_folder}/{nr_classes}_classes/{model_name}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad_cam_dir(nr_classes, criteria):\n",
        "  return f'{root_folder}/{grad_cam_folder}/{nr_classes}_classes/{criteria}'"
      ],
      "metadata": {
        "id": "tVta8SOruYNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_dir_gradcam(criteria, model_name):\n",
        "  return f'{root_folder}/{grad_cam_folder}/{criteria}/{model_name}'"
      ],
      "metadata": {
        "id": "XQnlkmjloyg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrbhPJjmaZ2E"
      },
      "source": [
        "## Setting random seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDEPY-Vpasx9"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suRurW5rqdzH"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwouZcsGqfL1"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    data_dir = get_dataset_dir(nr_classes)\n",
        "    train_dir = os.path.join(data_dir, 'train')\n",
        "    val_dir = os.path.join(data_dir, 'val')\n",
        "    test_dir = os.path.join(data_dir, 'test')\n",
        "    calib_dit = os.path.join(data_dir, 'calib')\n",
        "    num_classes = nr_classes\n",
        "    max_samples_per_class = 1700 # undersampling for dataset imbalance\n",
        "    batch_size = 28\n",
        "    num_epochs = 60\n",
        "    shuffle = True\n",
        "    feature_extract = False\n",
        "    use_pretrained = True\n",
        "    learning_rate = 0.0001\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    epsilon = 0.05 # 95% confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_7jf6v_rjFn"
      },
      "source": [
        "## Data transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H4gE5xMrim4"
      },
      "outputs": [],
      "source": [
        "def get_transforms(model_name, dataset, processor=None):\n",
        "  if not processor:\n",
        "    image_size = (224, 224) if (model_name != 'inception_v3') else (299, 299)\n",
        "    normalization_mean = [0.485, 0.456, 0.406]\n",
        "    normalization_std = [0.229, 0.224, 0.225]\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    image_size = processor.crop_size['height']\n",
        "    normalization_mean = processor.image_mean\n",
        "    normalization_std = processor.image_std\n",
        "  else:\n",
        "    image_size = processor.size['height']\n",
        "    normalization_mean = processor.image_mean\n",
        "    normalization_std = processor.image_std\n",
        "\n",
        "  data_transforms = {\n",
        "      'train': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'val': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'test': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ]),\n",
        "      'calib': transforms.Compose([\n",
        "          transforms.Resize(image_size),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(normalization_mean, normalization_std)\n",
        "      ])\n",
        "  }\n",
        "\n",
        "  return data_transforms[dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-L6PCsXNGpm"
      },
      "source": [
        "### Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAslNFGLNJKO"
      },
      "outputs": [],
      "source": [
        "def undersample_dataset(dataset):\n",
        "  # organize indices by class\n",
        "  class_indices = defaultdict(list)\n",
        "  for idx, (img_path, label) in enumerate(dataset['train'].imgs):\n",
        "    class_indices[label].append(idx)\n",
        "\n",
        "  # limit the number of samples per class\n",
        "  limited_indices = {}\n",
        "  for label, indices in class_indices.items():\n",
        "    limited_indices[label] = random.sample(indices, min(len(indices), Config.max_samples_per_class))\n",
        "\n",
        "  limited_train_dataset = Subset(dataset['train'], sum(limited_indices.values(), []))\n",
        "  dataset['train'] = limited_train_dataset\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nele5AD7YJaI"
      },
      "source": [
        "## Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K7x1D2TYL8D"
      },
      "outputs": [],
      "source": [
        "def load_dataset(model_name, processor=None, batch_size=Config.batch_size):\n",
        "  dataset = {\n",
        "      x: datasets.ImageFolder(os.path.join(Config.data_dir, x), get_transforms(model_name, x, processor))\n",
        "      for x in ['train', 'val', 'test', 'calib']\n",
        "  }\n",
        "\n",
        "  # applying undersampling\n",
        "  dataset = undersample_dataset(dataset)\n",
        "\n",
        "  dataloaders = {}\n",
        "  for x in ['train', 'val', 'test', 'calib']:\n",
        "    shuffle = True if x == 'train' else False\n",
        "    dataloaders[x] = DataLoader(\n",
        "        dataset[x],\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "  dataset_sizes = {\n",
        "      x: len(dataset[x])\n",
        "      for x in ['train', 'val', 'test', 'calib']\n",
        "  }\n",
        "\n",
        "  return dataloaders, dataset_sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqP9tvSomN3Z"
      },
      "source": [
        "## Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBSzCJSFmWnh"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, verbose=True, loss_function='cross_entropy'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.min_delta = min_delta\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.loss_function = loss_function\n",
        "\n",
        "    def __call__(self, val_loss, model, model_name):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(val_loss, model, model_name)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, model_name):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.4f} --> {val_loss:.4f}).  Saving model ...')\n",
        "        model_file = f'{model_name}_{self.loss_function}.pth'\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PM1rnv_hXeu"
      },
      "source": [
        "## Getting model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo3x2DnhOqll"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name):\n",
        "  model = None\n",
        "  if model_name == 'resnet34':\n",
        "    model = models.resnet34(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'resnet50':\n",
        "    model = models.resnet50(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'resnet101':\n",
        "    model = models.resnet101(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'vgg16':\n",
        "    model = models.vgg16(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'vgg19':\n",
        "    model = models.vgg19(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'densenet121':\n",
        "    model = models.densenet121(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'densenet169':\n",
        "    model = models.densenet169(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'inception_v3':\n",
        "    model = models.inception_v3(pretrained=Config.use_pretrained)\n",
        "  elif model_name == 'swin_b':\n",
        "    model = models.swin_b(pretrained=Config.use_pretrained)\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    model = DeiTForImageClassification.from_pretrained(model_name, num_labels=Config.num_classes, ignore_mismatched_sizes=True)\n",
        "  elif model_name == 'maxvit_t':\n",
        "    model = models.maxvit_t(pretrained=Config.use_pretrained)\n",
        "  elif ('davit' in model_name) or ('gcvit' in model_name):\n",
        "    model = timm.create_model(model_name, pretrained=Config.use_pretrained, num_classes=Config.num_classes)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOTLWJRsQMzM"
      },
      "outputs": [],
      "source": [
        "def set_fc_layer(model, model_name, loss_function):\n",
        "  num_classes = Config.num_classes if loss_function == 'cross_entropy' else Config.num_classes-1\n",
        "\n",
        "  if 'resnet' in model_name:\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  elif 'vgg' in model_name:\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "  elif 'densenet' in model_name:\n",
        "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
        "  elif 'inception' in model_name:\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "  elif 'swin' in model_name:\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "  elif 'facebook/deit' in model_name:\n",
        "    model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n",
        "  elif 'maxvit_t' in model_name:\n",
        "    model.classifier[5] = nn.Linear(model.classifier[5].in_features, num_classes)\n",
        "  elif 'davit' in model_name:\n",
        "    model.head.fc = nn.Linear(model.head.fc.in_features, num_classes)\n",
        "  elif 'gcvit' in model_name:\n",
        "    model.head.fc = nn.Linear(model.head.fc.in_features, num_classes)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid model name: {model_name}\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjOWgYSbhZL2"
      },
      "outputs": [],
      "source": [
        "def init_model(model_name, loss_function='cross_entropy', feature_extract=Config.feature_extract):\n",
        "  model = get_model(model_name)\n",
        "  model = set_fc_layer(model, model_name, loss_function)\n",
        "\n",
        "  if feature_extract:\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'fc' in name:\n",
        "        param.requires_grad = True\n",
        "      elif 'layer4' in name:\n",
        "        param.requires_grad = True\n",
        "      elif 'classifier' in name:\n",
        "        param.requires_grad = True\n",
        "      else:\n",
        "        param.requires_grad = False\n",
        "  else:\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    print(f\"{name}: {'trainable' if param.requires_grad else 'frozen'}\")\n",
        "\n",
        "  model = model.to(Config.device)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McAy9X_Hr4XN"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BAV9KdhQrpJ"
      },
      "outputs": [],
      "source": [
        "def save_in_google_drive(model_name, file_name):\n",
        "  dest_folder = f\"{root_folder}/{results_folder}/{nr_classes}_classes/{model_name}\"\n",
        "  os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "  if file_name.endswith('.pth'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}/{file_name}\")\n",
        "  elif file_name.endswith('.png'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}/{file_name}\")\n",
        "  elif file_name.endswith('.json'):\n",
        "    shutil.copy(file_name, f\"{dest_folder}{file_name}\")\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid file type: {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqBDiKQ1E_F"
      },
      "outputs": [],
      "source": [
        "def plot_and_save_training_curves(model_name, loss_function, train_losses, val_losses, train_accs, val_accs):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(train_losses, label='Erro de treinamento')\n",
        "    ax1.plot(val_losses, label='Erro de validação')\n",
        "    ax1.set_title('Erro de treinamento e validação')\n",
        "    ax1.set_xlabel('Época')\n",
        "    ax1.set_ylabel('Erro')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(train_accs, label='Acurácia de treinamento')\n",
        "    ax2.plot(val_accs, label='Acurácia de validação')\n",
        "    ax2.set_title('Acurácia de treinamento e validação')\n",
        "    ax2.set_xlabel('Época')\n",
        "    ax2.set_ylabel('Acurácia')\n",
        "    ax2.legend()\n",
        "\n",
        "    # save the chart to a file in colab\n",
        "    file_name = f'{model_name}_curves_{loss_function}.png'\n",
        "    plt.savefig(file_name)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew2aqdi-itIS"
      },
      "outputs": [],
      "source": [
        "def save_results(base_file_name, results):\n",
        "  sao_paulo_tz = pytz.timezone('America/Sao_Paulo')\n",
        "  timestamp = datetime.now(sao_paulo_tz).strftime('%Y-%m-%d_%H-%M-%S')\n",
        "  filename = f'{base_file_name}_{timestamp}.json'\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "  save_in_google_drive('', filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Aklw4az5alh"
      },
      "outputs": [],
      "source": [
        "def get_model_processor(model_name, model_src):\n",
        "  if model_src == 'pytorch':\n",
        "    return None\n",
        "  if model_name == 'facebook/deit-base-distilled-patch16-224':\n",
        "    return AutoFeatureExtractor.from_pretrained(model_name)\n",
        "  # return ViTImageProcessor.from_pretrained(model_name)\n",
        "  return AutoImageProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLnn-H_sUwVX"
      },
      "outputs": [],
      "source": [
        "def get_results_from_json_file():\n",
        "  file_path = '/content/classification_report_2025-06-20_02-40-18.json'\n",
        "  if not os.path.exists(file_path):\n",
        "    return {}\n",
        "\n",
        "  with open(file_path, 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQRb8sh1wbik"
      },
      "outputs": [],
      "source": [
        "def get_classes_to_exclude():\n",
        "  if nr_classes == 5:\n",
        "    return []\n",
        "  elif nr_classes == 4:\n",
        "    return [1]\n",
        "  elif nr_classes == 3:\n",
        "    return [0,1]\n",
        "  elif nr_classes == 2:\n",
        "    return [0,1,2]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid number of classes: {nr_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cm_labels():\n",
        "  if nr_classes == 5:\n",
        "    return None\n",
        "  elif nr_classes == 4:\n",
        "    return [\"0\", \"2\", \"3\", \"4\"]\n",
        "  elif nr_classes == 3:\n",
        "    return [\"2\", \"3\", \"4\"]\n",
        "  elif nr_classes == 2:\n",
        "    return [\"0\", \"1\"]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid number of classes: {nr_classes}\")"
      ],
      "metadata": {
        "id": "lGv4JYXd0YNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po4JsPZncz3X"
      },
      "outputs": [],
      "source": [
        "def sanitize_model_name(model_name):\n",
        "  return model_name.replace('/', '-').replace('.', '-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hd21PxGrvSl"
      },
      "outputs": [],
      "source": [
        "def load_model(model, path, map_location='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "  print(path)\n",
        "  model.load_state_dict(torch.load(path, map_location=map_location))\n",
        "  model.eval()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_model_labels(y_true, y_pred, labels):\n",
        "  if labels is None:\n",
        "    return y_true, y_pred\n",
        "  return [labels[i] for i in y_true], [labels[i] for i in y_pred]"
      ],
      "metadata": {
        "id": "O0mirvox44Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqZ7ZaTXxqqW"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_time(model, dataloader, warmup=5, repeat=50):\n",
        "  model.eval()\n",
        "\n",
        "  inputs, _ = next(iter(dataloader))\n",
        "  inputs = inputs.to(Config.device)\n",
        "\n",
        "  for _ in range(warmup):\n",
        "    with torch.no_grad():\n",
        "      _ = model(inputs)\n",
        "\n",
        "  start = time.time()\n",
        "  for _ in range(repeat):\n",
        "    with torch.no_grad():\n",
        "      _ = model(inputs)\n",
        "  end = time.time()\n",
        "\n",
        "  total_time = end - start\n",
        "  avg_inference_time = total_time / repeat\n",
        "  time_per_sample = avg_inference_time / inputs.shape[0]\n",
        "\n",
        "  print(f\"Inferência média por batch: {avg_inference_time:.6f} segundos\")\n",
        "  print(f\"Inferência média por amostra: {time_per_sample:.6f} segundos\")\n",
        "  return avg_inference_time, time_per_sample"
      ],
      "metadata": {
        "id": "qBXSESKjHq6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXEEcchLogVs"
      },
      "outputs": [],
      "source": [
        "def gen_confusion_matrix(model_name, loss_function, y_true, y_pred, labels=None):\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  if labels is None:\n",
        "    labels = 'auto'\n",
        "\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "  plt.title('Matriz de Confusão')\n",
        "  plt.xlabel('Predição')\n",
        "  plt.ylabel('Real')\n",
        "  file_name = f'{model_name}_cm_{loss_function}.png'\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLNwxiAHqHY_"
      },
      "outputs": [],
      "source": [
        "def gen_classification_report(model_name, loss_function, y_true, y_pred, classes_to_exclude):\n",
        "  # calculate overall metrics\n",
        "  kappa = cohen_kappa_score(y_true, y_pred)\n",
        "  qwk = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "  report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    # target_names=[f'KL{i}' for i in range(5) if i not in classes_to_exclude],\n",
        "    target_names=[f'{i}' for i in range(5) if i not in classes_to_exclude],\n",
        "    output_dict=True\n",
        "  )\n",
        "  report['kappa'] = kappa\n",
        "  report['qwk'] = qwk\n",
        "  report['mae'] = mae\n",
        "\n",
        "  return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDMPw6NFyzkG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, model_src, model_name, loss_function, test_loader, classes_to_exclude=[], cm_labels=None):\n",
        "  model.eval()\n",
        "  all_preds, all_labels = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  all_preds = np.array(all_preds)\n",
        "  all_labels = np.array(all_labels)\n",
        "\n",
        "  all_labels, all_preds = map_model_labels(all_labels, all_preds, cm_labels)\n",
        "  gen_confusion_matrix(model_name, loss_function, all_labels, all_preds, cm_labels)\n",
        "\n",
        "  return gen_classification_report(model_name, loss_function, all_labels, all_preds, classes_to_exclude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5UxUArOUx-M"
      },
      "outputs": [],
      "source": [
        "def run_model_evaluation():\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  for model_name, model_src in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_src)\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "    model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "    for loss_function in ['cross_entropy', 'corn']:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_name_sanitized)\n",
        "      file_name = f\"{model_name_sanitized}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "      print(path)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "      report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['test'], classes_to_exclude, cm_labels)\n",
        "      print(json.dumps(report, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNcZ2eOtUzPW"
      },
      "source": [
        "### Run evaluation from trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Um_rjJqXxXK"
      },
      "outputs": [],
      "source": [
        "# run_model_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy6rFMkkrlZY"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvtBygmxvHKJ"
      },
      "outputs": [],
      "source": [
        "def print_training_time(start_time, end_time):\n",
        "  total_time = end_time - start_time\n",
        "  print(f\"\\nTraining Time: {total_time / 60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a2Pu0_khmWd"
      },
      "outputs": [],
      "source": [
        "def compute_loss(outputs, labels, criterion, loss_function):\n",
        "  if loss_function == 'cross_entropy':\n",
        "    return criterion(outputs, labels)\n",
        "  elif loss_function == 'corn':\n",
        "    return criterion(outputs, labels, num_classes=Config.num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbj0-w6Bh1jf"
      },
      "outputs": [],
      "source": [
        "def get_predictions(outputs, loss_function):\n",
        "  if loss_function == 'cross_entropy':\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "  elif loss_function == 'corn':\n",
        "    preds = corn_label_from_logits(outputs)\n",
        "  return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aDtvKDNdTOE"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, datasize, loss_function, criterion, optimizer, model_name, model_src, device):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      if model_name == 'inception_v3':\n",
        "        outputs, aux_logits = outputs.logits, outputs.aux_logits\n",
        "        loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "        aux_loss = compute_loss(aux_logits, labels, criterion, loss_function)\n",
        "        loss += 0.4 * aux_loss\n",
        "      else:\n",
        "        loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / datasize\n",
        "  epoch_acc = running_corrects.double() / datasize\n",
        "  return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhtQVhCIigds"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, dataloader, datasize, loss_function, criterion, optimizer, model_src, device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      loss = compute_loss(outputs, labels, criterion, loss_function)\n",
        "      preds = get_predictions(outputs, loss_function)\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  epoch_loss = running_loss / datasize\n",
        "  epoch_acc = running_corrects.double() / datasize\n",
        "  return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N24lqdciiIMD"
      },
      "outputs": [],
      "source": [
        "def train_model(model_metadata):\n",
        "  # destructuring arguments\n",
        "  model, model_src, model_name = (\n",
        "    model_metadata['model'],\n",
        "    model_metadata['model_src'],\n",
        "    model_metadata['model_name']\n",
        "  )\n",
        "  dataloaders, dataset_sizes = (\n",
        "    model_metadata['dataloaders'],\n",
        "    model_metadata['dataset_sizes']\n",
        "  )\n",
        "  loss_function, criterion, optimizer, scheduler = (\n",
        "    model_metadata['loss_function'],\n",
        "    model_metadata['criterion'],\n",
        "    model_metadata['optimizer'],\n",
        "    model_metadata['scheduler']\n",
        "  )\n",
        "\n",
        "  best_model_wts = model.state_dict()\n",
        "  best_acc = 0.0\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "  early_stopping = EarlyStopping(patience=5, verbose=True, loss_function=loss_function)\n",
        "  start_time = time.time()\n",
        "  print(f\"loss function={loss_function}\")\n",
        "\n",
        "  for epoch in range(1, Config.num_epochs+1):\n",
        "    print(f\"Epoch {epoch}/{Config.num_epochs}\" + \"-\"*10)\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, dataloaders['train'], dataset_sizes['train'], loss_function, criterion, optimizer, model_name, model_src, Config.device)\n",
        "    val_loss, val_acc = validate_one_epoch(model, dataloaders['val'], dataset_sizes['val'], loss_function, criterion, optimizer, model_src, Config.device)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
        "\n",
        "    early_stopping(val_loss, model, model_name)\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "      best_acc = val_acc\n",
        "      best_model_wts = model.state_dict()\n",
        "\n",
        "  end_time = time.time()\n",
        "  print_training_time(start_time, end_time)\n",
        "  print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, train_losses, val_losses, train_accs, val_accs, end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cEBqCCg-ktjE"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# def train_model():\n",
        "#   # destructuring arguments\n",
        "#   # model, model_src, model_name = (\n",
        "#   #   model_data['model'],\n",
        "#   #   model_data['model_src'],\n",
        "#   #   model_data['model_name']\n",
        "#   # )\n",
        "#   # dataloaders, dataset_sizes = (\n",
        "#   #   model_data['dataloaders'],\n",
        "#   #   model_data['dataset_sizes']\n",
        "#   # )\n",
        "#   # loss_function, criterion, optimizer, scheduler = (\n",
        "#   #   model_data['loss_function'],\n",
        "#   #   model_data['criterion'],\n",
        "#   #   model_data['optimizer'],\n",
        "#   #   model_data['scheduler']\n",
        "#   # )\n",
        "#   # num_epochs = model_data['num_epochs']\n",
        "\n",
        "#   best_model_wts = model.state_dict()\n",
        "#   best_acc = 0.0\n",
        "#   train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "#   early_stopping = EarlyStopping(patience=5, verbose=True, loss_function=loss_function)\n",
        "#   start_time = time.time()\n",
        "\n",
        "#   for epoch in range(1, Config.num_epochs+1):\n",
        "#     print(f\"Epoch {epoch}/{Config.num_epochs}\" + \"-\"*10)\n",
        "\n",
        "#     for phase in ['train', 'val']:\n",
        "#       if phase == 'train':\n",
        "#         model.train()\n",
        "#       else:\n",
        "#         model.eval()\n",
        "\n",
        "#       running_loss = 0.0\n",
        "#       running_corrects = 0\n",
        "\n",
        "#       for inputs, labels in tqdm(dataloaders[phase]):\n",
        "#         inputs = inputs.to(Config.device)\n",
        "#         labels = labels.to(Config.device)\n",
        "\n",
        "#         # clear the gradients of all optimized parameters\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         with torch.set_grad_enabled(phase == 'train'):\n",
        "#           # forward pass: obtain the model predictions for the input data\n",
        "#           outputs = model(inputs)\n",
        "#           outputs = outputs.logits if model_src == 'hugging_face' else outputs\n",
        "\n",
        "#           if model_name == 'inception_v3' and phase == 'train':\n",
        "#             _, preds = torch.max(outputs.logits, 1) if loss_function == 'cross_entropy' else (None, corn_label_from_logits(outputs.logits))\n",
        "\n",
        "#             # compute the loss between the model predictions and the true labels\n",
        "#             loss = criterion(outputs.logits, labels) if loss_function == 'cross_entropy' else criterion(outputs.logits, labels, num_classes=Config.num_classes)\n",
        "#           else:\n",
        "#             _, preds = torch.max(outputs, 1) if loss_function == 'cross_entropy' else (None, corn_label_from_logits(outputs))\n",
        "\n",
        "#             # compute the loss between the model predictions and the true labels\n",
        "#             loss = criterion(outputs, labels) if loss_function == 'cross_entropy' else criterion(outputs, labels, num_classes=Config.num_classes)\n",
        "\n",
        "#           if phase == 'train':\n",
        "#             if model_name == 'inception_v3':\n",
        "#               aux_logits = outputs.aux_logits\n",
        "#               aux_loss = criterion(aux_logits, labels) if loss_function == 'cross_entropy' else criterion(aux_logits, labels, num_classes=Config.num_classes)\n",
        "#               loss += 0.4 * aux_loss\n",
        "\n",
        "#             # backward pass: compute gradients of the loss with respect to model parameters\n",
        "#             loss.backward()\n",
        "\n",
        "#             # update the model parameters using the computed gradients\n",
        "#             optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item() * inputs.size(0)\n",
        "#         running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "#       epoch_loss = running_loss / dataset_sizes[phase]\n",
        "#       epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "#       if phase == 'train':\n",
        "#         scheduler.step()\n",
        "#         train_losses.append(epoch_loss)\n",
        "#         train_accs.append(epoch_acc.item())\n",
        "#       else:\n",
        "#         val_losses.append(epoch_loss)\n",
        "#         val_accs.append(epoch_acc.item())\n",
        "\n",
        "#       print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "#       # Early stopping\n",
        "#       if phase == 'val':\n",
        "#         early_stopping(epoch_loss, model, model_name)\n",
        "#         if early_stopping.early_stop:\n",
        "#           print(\"Early stopping\")\n",
        "#           end_time = time.time()\n",
        "#           total_time = end_time - start_time\n",
        "#           print_training_time(start_time, end_time)\n",
        "#           print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "#           model.load_state_dict(best_model_wts)\n",
        "#           return model, train_losses, val_losses, train_accs, val_accs, total_time\n",
        "\n",
        "#       if phase == 'val' and epoch_acc > best_acc:\n",
        "#         best_acc = epoch_acc\n",
        "#         best_model_wts = model.state_dict()\n",
        "\n",
        "#     print(\"\")\n",
        "\n",
        "#   # record the end time\n",
        "#   end_time = time.time()\n",
        "#   total_time = end_time - start_time\n",
        "#   print_training_time(start_time, end_time)\n",
        "\n",
        "#   print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "#   model.load_state_dict(best_model_wts)\n",
        "\n",
        "#   return model, train_losses, val_losses, train_accs, val_accs, total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model's list"
      ],
      "metadata": {
        "id": "dlNhv3x2tS3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZVcpzi4w26Q"
      },
      "outputs": [],
      "source": [
        "models_list = {\n",
        "    'resnet34': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet34',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'resnet50': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet50',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'resnet101': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'resnet101',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'vgg16': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'vgg16',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'vgg19': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'vgg19',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'densenet121': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'densenet121',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'densenet169': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'densenet169',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'inception_v3': {\n",
        "        'type': 'cnn',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'inception_v3',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'facebook/deit-base-distilled-patch16-224': {\n",
        "        'type': 'vit',\n",
        "        'source': 'hugging_face',\n",
        "        'shortname': 'facebook-deit-base-distilled-patch16-224',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'davit_base.msft_in1k': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'davit_base-msft_in1k',\n",
        "        'skip': False,\n",
        "    },\n",
        "    'maxvit_t': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'maxvit_t',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'gcvit_base.in1k': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'gcvit_base-in1k',\n",
        "        'skip': True,\n",
        "    },\n",
        "    'swin_b': {\n",
        "        'type': 'vit',\n",
        "        'source': 'pytorch',\n",
        "        'shortname': 'swin_b',\n",
        "        'skip': True,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_functions = ['cross_entropy', 'corn']"
      ],
      "metadata": {
        "id": "4r6XnjfE02Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ptFfpa4nuk"
      },
      "outputs": [],
      "source": [
        "def run_training():\n",
        "  results = get_results_from_json_file()\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  for model_name, model_src in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_src)\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "    model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "    results[model_name_sanitized] = {}\n",
        "\n",
        "    for loss_function in ['cross_entropy', 'corn']:\n",
        "      model = init_model(model_name, loss_function)\n",
        "      criterion = nn.CrossEntropyLoss() if loss_function == 'cross_entropy' else corn_loss\n",
        "      optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "      scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "      # defining model metadata dict\n",
        "      model_metadata = {\n",
        "          'model': model,\n",
        "          'model_src': model_src,\n",
        "          'model_name': model_name_sanitized,\n",
        "          'dataloaders': dataloaders,\n",
        "          'dataset_sizes': dataset_sizes,\n",
        "          'loss_function': loss_function,\n",
        "          'criterion': criterion,\n",
        "          'optimizer': optimizer,\n",
        "          'scheduler': scheduler\n",
        "      }\n",
        "\n",
        "      # train the model using transfer learning\n",
        "      model, train_losses, val_losses, train_accs, val_accs, train_time = train_model(model_metadata)\n",
        "\n",
        "      save_in_google_drive(model_name_sanitized, f'{model_name_sanitized}_{loss_function}.pth')\n",
        "      plot_and_save_training_curves(model_name_sanitized, loss_function, train_losses, val_losses, train_accs, val_accs)\n",
        "\n",
        "      # evaluate the model using test dataloader\n",
        "      report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['test'], classes_to_exclude, cm_labels)\n",
        "\n",
        "      # get model complexity info\n",
        "      flops, params = get_model_complexity_info(model, (3, 224, 224), as_strings=True, backend='pytorch', print_per_layer_stat=False)\n",
        "\n",
        "      report['train_time'] = train_time\n",
        "      report['flops'] = flops\n",
        "      report['params'] = params\n",
        "\n",
        "      results[model_name_sanitized][loss_function] = report\n",
        "\n",
        "  # save report\n",
        "  save_results('classification_report', results)\n",
        "  print(f'Training completed. Reports saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owtVL4sXXKnU"
      },
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqB3rYOkVY8z"
      },
      "outputs": [],
      "source": [
        "# run_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna"
      ],
      "metadata": {
        "id": "0rO4kF4b7nbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  results = get_results_from_json_file()\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "\n",
        "  # === Suggest hyperparameters with Optuna ===\n",
        "  model_name = trial.suggest_categorical(\"model_name\", [\"densenet169\", \"davit_base.msft_in1k\", \"densenet121\", \"gcvit_base.in1k\", \"maxvit_t\"])\n",
        "  loss_function = trial.suggest_categorical(\"loss_function\", [\"cross_entropy\", \"corn\"])\n",
        "  learning_rate = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
        "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "  feature_extract = trial.suggest_categorical(\"feature_extract\", [True, False])\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [16, 28, 32, 64])\n",
        "\n",
        "  model_src = \"pytorch\"\n",
        "  model_name_sanitized = sanitize_model_name(model_name)\n",
        "\n",
        "  # === Init model ===\n",
        "  processor = get_model_processor(model_name, model_src)\n",
        "  dataloaders, dataset_sizes = load_dataset(model_name, processor, batch_size=batch_size)\n",
        "  model = init_model(model_name, loss_function, feature_extract)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss() if loss_function == 'cross_entropy' else corn_loss\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "  model_metadata = {\n",
        "    'model': model,\n",
        "    'model_src': model_src,\n",
        "    'model_name': model_name_sanitized,\n",
        "    'dataloaders': dataloaders,\n",
        "    'dataset_sizes': dataset_sizes,\n",
        "    'loss_function': loss_function,\n",
        "    'criterion': criterion,\n",
        "    'optimizer': optimizer,\n",
        "    'scheduler': scheduler\n",
        "  }\n",
        "\n",
        "  # === Training ===\n",
        "  model, train_losses, val_losses, train_accs, val_accs, train_time = train_model(model_metadata)\n",
        "\n",
        "  # === Evaluate ===\n",
        "  report = evaluate_model(model, model_src, model_name_sanitized, loss_function, dataloaders['val'], classes_to_exclude, cm_labels)\n",
        "\n",
        "  # === Free GPU\n",
        "  del model\n",
        "  del dataloaders\n",
        "  del model_metadata\n",
        "  gc2.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Returning the metric to maximize\n",
        "  return report['qwk']"
      ],
      "metadata": {
        "id": "WAIvUN4G7qZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run optimization"
      ],
      "metadata": {
        "id": "OI1ptu97CjWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optuna():\n",
        "  study = optuna.create_study(direction=\"maximize\")\n",
        "  study.optimize(objective, n_trials=30, timeout=36000) # 10h\n",
        "\n",
        "  print(\"Melhores hiperparâmetros encontrados:\")\n",
        "  print(study.best_params)\n",
        "\n",
        "  # (Opcional) salvar\n",
        "  joblib.dump(study, \"optuna_study.pkl\")"
      ],
      "metadata": {
        "id": "RDwll1xACtPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_optuna()"
      ],
      "metadata": {
        "id": "d3g9itljDEed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLTydxfrtF-"
      },
      "source": [
        "# Conformal prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate non-conformity scores"
      ],
      "metadata": {
        "id": "PJzK33tz4ygr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xe_calc_conformity_score(logits, y):\n",
        "  probs = F.softmax(logits, dim=0)\n",
        "  sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "  score = 0.0\n",
        "\n",
        "  for k, idx in enumerate(sorted_indices):\n",
        "    if idx.item() == y:\n",
        "      score = sorted_probs[:k+1].sum().item()\n",
        "      return score"
      ],
      "metadata": {
        "id": "PSGOMGcokXd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corn_calc_conformity_score(logits, y):\n",
        "  probs = torch.sigmoid(logits)\n",
        "  score = 0.0\n",
        "\n",
        "  for k in range(nr_classes - 1):\n",
        "    p = probs[k].item()\n",
        "    if k >= y:\n",
        "      score += p\n",
        "    else:\n",
        "      score += 1 - p\n",
        "\n",
        "  return score"
      ],
      "metadata": {
        "id": "vwv9qw2GBaP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYB1PsYWuCDw"
      },
      "outputs": [],
      "source": [
        "def calc_nonconformity_scores(model, model_src, calib_loader, loss_function):\n",
        "  model.eval()\n",
        "  scores = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in tqdm(calib_loader):\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits # shape: (B, K-1) para CORN\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        y = labels[i].item()\n",
        "        logits_i = outputs[i]\n",
        "\n",
        "        if loss_function == 'corn':\n",
        "          scores.append(corn_calc_conformity_score(logits_i, y))\n",
        "        else:\n",
        "          scores.append(xe_calc_conformity_score(logits_i, y))\n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corn\n",
        "def calc_per_threshold_scores(model, model_src, calib_loader):\n",
        "  model.eval()\n",
        "  per_threshold_scores = [[] for _ in range(nr_classes - 1)] # K-1 thresholds\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in tqdm(calib_loader):\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits # shape: (B, K-1)\n",
        "\n",
        "      probs = torch.sigmoid(outputs)\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        y = labels[i].item()\n",
        "        for k in range(nr_classes - 1):\n",
        "          p = probs[i, k].item()\n",
        "          if y > k:\n",
        "            score = 1 - p\n",
        "          else:\n",
        "            score = p\n",
        "          per_threshold_scores[k].append(score)\n",
        "\n",
        "  return per_threshold_scores"
      ],
      "metadata": {
        "id": "bt_f4yNu3zPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHpVD9E34uvp"
      },
      "outputs": [],
      "source": [
        "def get_prediction_set(probs, q_hat):\n",
        "  sorted_indices = torch.argsort(probs, descending=True)\n",
        "  pred_set = []\n",
        "  cumulative = 0\n",
        "\n",
        "  for idx in sorted_indices:\n",
        "    pred_set.append(idx.item())\n",
        "    cumulative += probs[idx].item()\n",
        "    if cumulative >= q_hat:\n",
        "      break\n",
        "  return pred_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction_interval(logits, q_hat):\n",
        "  probs = torch.sigmoid(logits) # shape: (K-1,)\n",
        "  score = 0.0\n",
        "  upper_class = len(probs)\n",
        "\n",
        "  for k in range(len(probs)):\n",
        "    score += 1 - probs[k].item()\n",
        "    if score > q_hat:\n",
        "      upper_class = k\n",
        "      break\n",
        "\n",
        "  return list(range(upper_class + 1))  # intervalo ordinal: [0, ..., upper_class]"
      ],
      "metadata": {
        "id": "AJ8uxFYO5kUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_interval_from_thresholds(logits, thresholds):\n",
        "  probs = torch.sigmoid(logits)\n",
        "  interval = []\n",
        "\n",
        "  for k, tau_k in enumerate(thresholds):\n",
        "    if 1 - probs[k].item() <= tau_k:\n",
        "        interval.append(k + 1)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "  return list(range(interval[-1] + 1)) if interval else [0]"
      ],
      "metadata": {
        "id": "Kie4xsW75sHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1HVivB3a5Yd"
      },
      "outputs": [],
      "source": [
        "def run_conformal_prediction(dataloader, model, model_src, thresholds, loss_function):\n",
        "  model.eval()\n",
        "  set_sizes = []\n",
        "  coverage = 0.0\n",
        "  total = 0\n",
        "\n",
        "  for inputs, labels in tqdm(dataloader):\n",
        "    inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      if model_src == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      for i in range(len(inputs)):\n",
        "        if loss_function == 'corn':\n",
        "          # pred_set = get_prediction_interval(outputs[i], q_hat)\n",
        "          pred_set = predict_interval_from_thresholds(outputs[i], thresholds)\n",
        "        else:\n",
        "          probs = F.softmax(outputs, dim=1)\n",
        "          pred_set = get_prediction_set(probs[i], thresholds[0])\n",
        "\n",
        "        true_label = labels[i].item()\n",
        "        print(f\"Predicted set: {pred_set}, True label: {true_label}\")\n",
        "        set_sizes.append(len(pred_set))\n",
        "        coverage += 1 if true_label in pred_set else 0\n",
        "        total += 1\n",
        "\n",
        "  coverage /= total\n",
        "  return set_sizes, coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DFu96qtKpQ0"
      },
      "outputs": [],
      "source": [
        "def plot_and_save_set_sizes(set_sizes, model_name, loss_function):\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.hist(set_sizes, bins=np.arange(min(set_sizes), max(set_sizes) + 2))\n",
        "  plt.xlabel('Tamanho do conjunto de predições')\n",
        "  plt.ylabel('Frequência')\n",
        "  plt.title('Distribuição do tamanho do conjunto de predições')\n",
        "\n",
        "  file_name = f'{model_name}_cp_{loss_function}.png'\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_name, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_thresholds(per_threshold_scores, epsilon):\n",
        "  thresholds = []\n",
        "  for scores_k in per_threshold_scores:\n",
        "    n = len(scores_k)\n",
        "    q_hat = np.quantile(scores_k, math.ceil((n+1)*(1-epsilon))/n)\n",
        "    thresholds.append(q_hat)\n",
        "  return thresholds"
      ],
      "metadata": {
        "id": "ay8fWPfQ5Sjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squash_set_sizes(set_sizes):\n",
        "  squashed_set_sizes = [0 for _ in range(nr_classes+1)]\n",
        "  for size in set_sizes:\n",
        "    squashed_set_sizes[size] += 1\n",
        "  return squashed_set_sizes"
      ],
      "metadata": {
        "id": "BIUNRT1sPP2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conformal_prediction():\n",
        "  results = {}\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    results[model_attr['shortname']] = {}\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      print(f\"model name: {model_attr['shortname']} - loss function: {loss_function}\")\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      thresholds = None\n",
        "\n",
        "      # calculating the non-confirmity scores\n",
        "      if loss_function == 'corn':\n",
        "        per_threshold_scores = calc_per_threshold_scores(model, model_attr['source'], dataloaders['calib'])\n",
        "        thresholds = compute_thresholds(per_threshold_scores, Config.epsilon)\n",
        "      else:\n",
        "        scores = calc_nonconformity_scores(model, model_attr['source'], dataloaders['calib'], loss_function)\n",
        "        thresholds = compute_thresholds([scores], Config.epsilon)\n",
        "\n",
        "      set_sizes, coverage = run_conformal_prediction(dataloaders['test'], model, model_attr['source'], thresholds, loss_function)\n",
        "\n",
        "      plot_and_save_set_sizes(set_sizes, model_attr['shortname'], loss_function)\n",
        "\n",
        "      results[model_attr['shortname']][loss_function] = {\n",
        "        'q_hat': thresholds,\n",
        "        'coverage': coverage,\n",
        "        'set_sizes': squash_set_sizes(set_sizes)\n",
        "      }\n",
        "\n",
        "  save_results('conformal_prediction', results)\n",
        "  print(f'Conformal prediction completed. Reports saved.')"
      ],
      "metadata": {
        "id": "e0lr5dJDJ3Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run conformal prediction"
      ],
      "metadata": {
        "id": "zSmRX7kJx1tg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5apgv2_vYA1n"
      },
      "outputs": [],
      "source": [
        "conformal_prediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Time"
      ],
      "metadata": {
        "id": "8ZkNjSezL4OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_inference_time():\n",
        "  classes_to_exclude = get_classes_to_exclude()\n",
        "  cm_labels = get_cm_labels()\n",
        "  results = {}\n",
        "\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    results[model_attr['shortname']] = {}\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      avg_inference_time, time_per_sample = measure_inference_time(model, dataloaders['test'], warmup=5, repeat=50)\n",
        "\n",
        "      results[model_attr['shortname']][loss_function] = {\n",
        "          'avg_inference_time': avg_inference_time,\n",
        "          'time_per_sample': time_per_sample\n",
        "      }\n",
        "\n",
        "  save_results('inference_time', results)\n",
        "  print(f'Inference time completed. Reports saved.')"
      ],
      "metadata": {
        "id": "MWQqKtj7L-6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run_model_inference_time()"
      ],
      "metadata": {
        "id": "AhQ_ug3KMoMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT7RzUMF7Al7"
      },
      "source": [
        "# Visualizing with Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grad_cam_criteria = {\n",
        "    'per-kl-criteria': 'per_kl_class'\n",
        "}"
      ],
      "metadata": {
        "id": "dciC0HUGwGe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingFaceWrapper(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x).logits"
      ],
      "metadata": {
        "id": "QQAoVfeVP-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CORNWrapper(nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model  # base model returning CORN logits\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)  # logits of shape [B, K-1]"
      ],
      "metadata": {
        "id": "tvkBmfxbALrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CORNOutputTarget:\n",
        "  def __init__(self, class_index, epsilon=1e-1):\n",
        "    self.class_index = class_index  # 0-based class index\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def __call__(self, model_output):\n",
        "    \"\"\"\n",
        "    CORN produces K-1 outputs.\n",
        "    We define a differentiable proxy score that encourages correct attribution.\n",
        "    \"\"\"\n",
        "    # Apply sigmoid to logits: shape [K-1]\n",
        "    print(model_output)\n",
        "    probs = torch.sigmoid(model_output)\n",
        "    print(\"probs:\", probs)\n",
        "    print(\"prob shape:\", probs.shape)\n",
        "\n",
        "    # Calculate pseudo-probability for class_index\n",
        "    # For class 0: P(class = 0) = 1 - sigmoid(logit_0)\n",
        "    # For class k: P(class = k) = sigmoid(logit_{k-1}) - sigmoid(logit_{k})\n",
        "    # if self.class_index == 0:\n",
        "    #     score = 1 - probs[0]\n",
        "    # elif self.class_index == probs.shape[0]:\n",
        "    #     score = probs[-1]\n",
        "    # else:\n",
        "    #     score = probs[self.class_index - 1] - probs[self.class_index]\n",
        "\n",
        "    if self.class_index == 0:\n",
        "      return 1 - probs[0]\n",
        "\n",
        "    return probs[:self.class_index].sum()\n",
        "    # return probs[:, self.class_index]"
      ],
      "metadata": {
        "id": "mWnBlwJrwpP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_image(image, model_name, processor):\n",
        "  if not processor:\n",
        "    image_size = (224, 224) if (model_name != 'inception_v3') else (299, 299)\n",
        "  elif ('facebook/deit' in model_name):\n",
        "    image_size = (processor.crop_size['height'], processor.crop_size['height'])\n",
        "  else:\n",
        "    image_size = (processor.size['height'], processor.size['height'])\n",
        "  return image.resize(image_size)"
      ],
      "metadata": {
        "id": "5mO9GXNFnIfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "  images = []\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.png'):\n",
        "      image_path = os.path.join(folder_path, filename)\n",
        "      images.append(image_path)\n",
        "  return images"
      ],
      "metadata": {
        "id": "YJ3cG1eoxfI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_tensor(img_path, model_name, processor):\n",
        "  img_pil = Image.open(img_path).convert('RGB')\n",
        "  image_resized = resize_image(img_pil, model_name, processor)\n",
        "  transform = get_transforms(model_name, 'test')\n",
        "  img_tensor = transform(image_resized).unsqueeze(0)\n",
        "  img_np = np.array(image_resized).astype(np.float32)/255.0\n",
        "  return img_tensor, img_np, img_pil"
      ],
      "metadata": {
        "id": "7_PiXR7msDrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_layer(model, model_name):\n",
        "  if model_name in ['resnet34', 'resnet50', 'resnet101']:\n",
        "    return [model.layer4[-1]]\n",
        "  elif model_name in ['vgg16', 'vgg19']:\n",
        "    return [model.features[-1]]\n",
        "  elif model_name in ['densenet121', 'densenet169']:\n",
        "    return [model.features[-1]]\n",
        "  elif model_name in ['inception_v3']:\n",
        "    return [model.Mixed_7c]\n",
        "  elif model_name in ['google/vit-base-patch16-224']:\n",
        "    return [model.model.vit.encoder.layer[-1].layernorm_before]\n",
        "  elif model_name in ['facebook/deit-base-distilled-patch16-224']:\n",
        "    return [model.model.deit.encoder.layer[-1].layernorm_before]\n",
        "  elif model_name in ['davit_base.msft_in1k']:\n",
        "    return [model.stages[3].blocks[0][1].norm1]\n",
        "  elif model_name in ['maxvit_t']:\n",
        "    return [model.blocks[-1].layers[-1].layers[-1].attn_layer[0]]\n",
        "  elif model_name in ['gcvit_base.in1k']:\n",
        "    return [model.stages[-1].blocks[-1].norm2]\n",
        "  elif model_name in ['swin_b']:\n",
        "    return [model.features[-1][-1].norm1]\n",
        "  else:\n",
        "    return None"
      ],
      "metadata": {
        "id": "_PLi8cj81S7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_transform(tensor):\n",
        "  # tensor shape: [batch_size, seq_len, hidden_dim]\n",
        "  # remove CLS token and reshape to 2D feature map\n",
        "\n",
        "  print(tensor.size())\n",
        "  # tensor: [B, H, N, C]\n",
        "  if tensor.dim() == 4:\n",
        "    B, H, W, C = tensor.shape\n",
        "    if H == 1: # maxvit_t\n",
        "      tensor = tensor.squeeze(1)\n",
        "    else:\n",
        "      return tensor.permute(0, 3, 1, 2)\n",
        "\n",
        "  B, seq_len, hidden_dim = tensor.size()\n",
        "\n",
        "  if seq_len == 197: # stardard vit with CLS\n",
        "    tensor = tensor[:, 1:, :]\n",
        "  elif seq_len == 198: # deit-distilled\n",
        "    tensor = tensor[:, 2:, :]\n",
        "  elif seq_len == 49: # davit, and others\n",
        "    pass # does not prepend CLS by default\n",
        "  elif seq_len == 77: # maxvit\n",
        "    tensor = tensor[:, 1:, :]\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid sequence length: {seq_len}\")\n",
        "\n",
        "  h = w = int(tensor.shape[1] ** 0.5)\n",
        "  if h * w != tensor.shape[1]:\n",
        "      raise ValueError(f\"Cannot reshape: {tensor.shape[1]} tokens is not a square\")\n",
        "\n",
        "  # reshape to [batch, hidden_dim, height, width]\n",
        "  return tensor.permute(0, 2, 1).reshape(B, hidden_dim, h, w)"
      ],
      "metadata": {
        "id": "P6WPXnn5zFiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_transform_swinb(tensor, height=7, width=7):\n",
        "  result = tensor[:, 1:,:].reshape(tensor.size(0), height, width, tensor.size(2))\n",
        "\n",
        "  # Bring the channels to the first dimension,\n",
        "  # like in CNNs.\n",
        "  result = result.transpose(2, 3).transpose(1, 2)\n",
        "  return result"
      ],
      "metadata": {
        "id": "HNDbRxvySki9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gradcam(model, model_attr, loss_function, target_layers, input_tensor, img_np, file_name):\n",
        "  input_tensor = input_tensor.to(Config.device)\n",
        "  input_tensor.requires_grad = True\n",
        "\n",
        "  model.eval()\n",
        "  model.zero_grad()\n",
        "\n",
        "  if model_attr.get('shortname') == 'swin_bo':\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers,\n",
        "                  reshape_transform=reshape_transform_swinb)\n",
        "  elif model_attr.get('type') == 'vit':\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers,\n",
        "                  reshape_transform=reshape_transform)\n",
        "  else:\n",
        "    cam = GradCAM(model=model,\n",
        "                  target_layers=target_layers)\n",
        "\n",
        "  if type(model) == HuggingFaceWrapper:\n",
        "    outputs = model.model(input_tensor).logits\n",
        "  else:\n",
        "    outputs = model(input_tensor)\n",
        "\n",
        "  pred_class = get_predictions(outputs, loss_function).item()\n",
        "  print(f\"Predicted class: {pred_class}\")\n",
        "\n",
        "  if loss_function == 'corn':\n",
        "    targets = [CORNOutputTarget(pred_class)]\n",
        "  else:\n",
        "    targets = [ClassifierOutputTarget(pred_class)]\n",
        "\n",
        "  grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
        "  cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "  plt.imshow(cam_image)\n",
        "  plt.axis('off')\n",
        "  plt.title(f'Escala KL predita: {pred_class}')\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_attr.get('shortname'), file_name)"
      ],
      "metadata": {
        "id": "C57y5atXUk2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_batch_gradcam(criteria_name, folder_path, model_name, model_attr, loss_function):\n",
        "  processor = get_model_processor(model_name, model_attr.get('source'))\n",
        "\n",
        "  image_paths = load_images_from_folder(folder_path)\n",
        "  if len(image_paths) == 0:\n",
        "    print(f\"Nenhuma imagem encontrada na pasta: {folder_path}\")\n",
        "    return\n",
        "\n",
        "  model = init_model(model_name, loss_function)\n",
        "  base_path = get_results_dir(nr_classes, model_attr.get('shortname'))\n",
        "  file_name = f\"{model_attr.get('shortname')}_{loss_function}.pth\"\n",
        "  path = os.path.join(base_path, file_name)\n",
        "  model = load_model(model, path)\n",
        "\n",
        "  if model_attr.get('source') == 'hugging_face':\n",
        "    model = HuggingFaceWrapper(model)\n",
        "\n",
        "  print(model)\n",
        "\n",
        "  target_layers = get_target_layer(model, model_name)\n",
        "  print(target_layers)\n",
        "\n",
        "  if target_layers is None:\n",
        "    print(f\"Nenhuma camada de destino encontrada para o modelo: {model_name}\")\n",
        "    return\n",
        "\n",
        "  for img_path in image_paths:\n",
        "    input_tensor, img_np, _ = image_to_tensor(img_path, model_name, processor)\n",
        "    file_name = f\"{criteria_name}_gradcam_loss_{loss_function}_{os.path.basename(img_path)}\"\n",
        "    run_gradcam(model, model_attr, loss_function, target_layers, input_tensor, img_np, file_name)"
      ],
      "metadata": {
        "id": "4mQgIQHGzBkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Grad-CAM"
      ],
      "metadata": {
        "id": "MkmbpC_DdQk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exec_grad_cam():\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "    for loss_function in loss_functions:\n",
        "      for criteria_name, criteria_path in grad_cam_criteria.items():\n",
        "        folder_path = get_grad_cam_dir(Config.num_classes, criteria_path)\n",
        "        run_batch_gradcam(criteria_name, folder_path, model_name, model_attr, loss_function)"
      ],
      "metadata": {
        "id": "a7zi1LkgyNk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exec_grad_cam()"
      ],
      "metadata": {
        "id": "B49KJTfT4ZAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlGIVKV6HWRF"
      },
      "outputs": [],
      "source": [
        "# gc2.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AUC-ROC curve"
      ],
      "metadata": {
        "id": "CKalIcmVDR0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def corn_probs(logits: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Converte logits do modelo treinado com CORN para distribuição de probabilidade por classe.\n",
        "  Supondo K classes, logits tem shape [B, K-1].\n",
        "  \"\"\"\n",
        "  prob_gt = torch.sigmoid(logits)\n",
        "  prob_le = 1.0 - prob_gt          # P(y ≤ k)\n",
        "\n",
        "  B, K_minus_1 = prob_gt.shape\n",
        "  K = K_minus_1 + 1\n",
        "\n",
        "  # Inicializa tensor de probabilidades por classe\n",
        "  probs = torch.zeros((B, K), device=logits.device)\n",
        "\n",
        "  # Primeira classe: P(y == 0) = 1 - P(y > 0)\n",
        "  probs[:, 0] = prob_le[:, 0]\n",
        "\n",
        "  # Classes intermediárias: P(y == k) = P(y > k-1) * (1 - P(y > k))\n",
        "  for k in range(1, K - 1):\n",
        "      probs[:, k] = prob_gt[:, k - 1] * prob_le[:, k]\n",
        "\n",
        "  # Última classe: P(y == K-1) = P(y > K-2)\n",
        "  probs[:, -1] = prob_gt[:, -1]\n",
        "\n",
        "  return probs  # shape [B, K]"
      ],
      "metadata": {
        "id": "JDEIkR-DVGhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_auc_roc(model, model_attr, test_loader, loss_function):\n",
        "  \"\"\"\n",
        "  Compute and plot the AUC-ROC curve for a PyTorch image classification model.\n",
        "\n",
        "  Parameters:\n",
        "  - model: Trained PyTorch model.\n",
        "  - dataloader: DataLoader with test/validation dataset.\n",
        "  - device: 'cuda' or 'cpu'.\n",
        "  - num_classes: Number of classes (e.g., 5 for KL grades 0-4).\n",
        "  \"\"\"\n",
        "\n",
        "  model.eval()\n",
        "  model.to(Config.device)\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(Config.device), labels.to(Config.device)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      if model_attr['source'] == 'hugging_face':\n",
        "        outputs = outputs.logits\n",
        "\n",
        "      if loss_function == 'corn':\n",
        "          preds = corn_probs(outputs)\n",
        "      else:\n",
        "        preds = F.softmax(outputs, dim=1)\n",
        "\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  all_preds = np.array(all_preds)\n",
        "  all_labels = np.array(all_labels)\n",
        "\n",
        "  # Stack all predictions and targets\n",
        "  y_scores = np.vstack(all_preds)\n",
        "  y_true = np.hstack(all_labels)\n",
        "\n",
        "  # Plot ROC curve for each class\n",
        "  plt.figure(figsize=(8, 6))\n",
        "\n",
        "  if Config.num_classes == 2:\n",
        "    # Binary classification\n",
        "    if y_scores.shape[1] == 1:\n",
        "      y_scores = y_scores[:, 0]\n",
        "    else:\n",
        "      y_scores = y_scores[:, 1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"(AUC = {roc_auc:.2f})\")\n",
        "  else:\n",
        "    # Multiclass case\n",
        "    assert y_scores.shape == (len(y_true), Config.num_classes), \\\n",
        "        f\"Expected y_scores shape ({len(y_true)}, {Config.num_classes}), got {y_scores.shape}\"\n",
        "\n",
        "    y_true_bin = label_binarize(y_true, classes=np.arange(Config.num_classes))\n",
        "    assert y_true_bin.shape == y_scores.shape, \\\n",
        "        f\"Shape mismatch: y_true_bin {y_true_bin.shape}, y_scores {y_scores.shape}\"\n",
        "\n",
        "    for i in range(Config.num_classes):\n",
        "      fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      plt.plot(fpr, tpr, lw=2, label=f\"KL {i} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "  # Random classifier line\n",
        "  plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Aleatório\")\n",
        "\n",
        "  plt.xlabel(\"FPR\")\n",
        "  plt.ylabel(\"TPR\")\n",
        "  plt.title(\"Curva AUC-ROC\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.grid(True)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  file_name = f\"{model_attr['shortname']}_auc_roc_{loss_function}.png\"\n",
        "  plt.savefig(file_name)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  save_in_google_drive(model_attr['shortname'], file_name)"
      ],
      "metadata": {
        "id": "N6RNYhNhQZx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_auc_roc():\n",
        "  for model_name, model_attr in models_list.items():\n",
        "    if model_attr.get('skip') == True:\n",
        "      continue\n",
        "\n",
        "    processor = get_model_processor(model_name, model_attr['source'])\n",
        "    dataloaders, dataset_sizes = load_dataset(model_name, processor)\n",
        "\n",
        "    for loss_function in loss_functions:\n",
        "      model = init_model(model_name, loss_function)\n",
        "\n",
        "      base_path = get_results_dir(nr_classes, model_attr['shortname'])\n",
        "      file_name = f\"{model_attr['shortname']}_{loss_function}.pth\"\n",
        "      path = os.path.join(base_path, file_name)\n",
        "\n",
        "      model = load_model(model, path)\n",
        "\n",
        "      plot_auc_roc(model, model_attr, dataloaders['test'], loss_function)"
      ],
      "metadata": {
        "id": "QTK8cQmGERxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run AUC-ROC"
      ],
      "metadata": {
        "id": "i-h7tj5jObGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run_auc_roc()"
      ],
      "metadata": {
        "id": "a_cqcDo6LGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92mNgiuaOdXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "8ZkNjSezL4OQ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}